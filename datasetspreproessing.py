# -*- coding: utf-8 -*-
"""datasetspreproessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FWxGseqSsv6Th6EhMTn3cEEAXtsLKya-

### Environment Setup

In this step, we install all the required Python libraries for preprocessing our dataset.  
- **datasets** is used to load the LEDGAR dataset from Hugging Face.  
- **transformers** provides tools for tokenization and model preparation (e.g., Legal-BERT).  
- **nltk**, **emoji**, and **contractions** help in text cleaning and normalization.  
- **nlpaug** is optionally used for data augmentation.  
- **scikit-learn** is used for vectorization and evaluation tasks.  

We also **mount Google Drive** to save and access files easily during preprocessing.
"""

# Install required packages
!pip install -q datasets transformers nltk emoji contractions nlpaug scikit-learn

# (optional) for data augmentations
!pip install -q nlpaug

# Mount Drive to save processed files
from google.colab import drive
drive.mount('/content/drive')

"""### Step 2: Importing Libraries and Initializing Tools

In this step, we import all the necessary Python libraries and initialize NLP tools required for text preprocessing.

- **os, re, string, pandas, numpy** → used for data handling and text manipulation.  
- **datasets** → to load the LEDGAR dataset from Hugging Face.  
- **tqdm** → adds progress bars for tracking preprocessing steps.  
- **nltk** → provides stopwords and lemmatization for text cleaning.  
- **emoji** and **contractions** → used to remove emojis and expand shortened words (e.g., "can't" → "cannot").  
- **scikit-learn (TfidfVectorizer)** → used later for vectorizing cleaned text data.  
- **transformers (AutoTokenizer)** → provides tokenization for models like Legal-BERT.  
- **nlpaug** → supports optional text data augmentation.  

Finally, we enable `tqdm.pandas()` to display progress bars when applying functions to pandas columns.

"""

import os, re, string
import pandas as pd
import numpy as np
from datasets import load_dataset
from tqdm import tqdm

# NLP helpers
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import emoji
import contractions

# For vectorization & model tokenizers
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer

# Optional augmentation
import nlpaug.augmenter.word as naw

tqdm.pandas()

"""### Step 3: Loading the LEDGAR Dataset

In this step, we load the **LEDGAR dataset** from the Hugging Face `lex_glue` benchmark, which is specifically designed for **legal text classification tasks**.

- We first try to load the dataset from **`coastalcph/lex_glue`** (recommended source).  
- If that fails, we attempt to load it directly using **`lex_glue`**.  
- The dataset contains three main splits: **train**, **validation**, and **test**.  
- Each record includes a **legal contract clause (text)** and its **label** indicating the clause type.  
- After loading, we convert each split into **Pandas DataFrames** for easier manipulation and preprocessing.  
- Finally, we print the dataset structure and check the size of the training data.

"""

# Option 1: load from coastalcph/lex_glue (recommended)
try:
    ds = load_dataset("coastalcph/lex_glue", "ledgar")
except Exception as e:
    print("Option 1 failed:", e)
    # Option 2: load by name if available
    ds = load_dataset("lex_glue", "ledgar")

# ds contains splits like 'train','validation','test' (or sometimes only train/val/test)
print(ds)
# Convert to pandas (merge splits if you want a single file)
df_train = pd.DataFrame(ds['train'])
df_val   = pd.DataFrame(ds['validation']) if 'validation' in ds else pd.DataFrame()
df_test  = pd.DataFrame(ds['test']) if 'test' in ds else pd.DataFrame()

# Quick peek
print("train shape:", df_train.shape)
df_train.head()

"""### Step 4: Combining Dataset Splits and Exploring Data

In this step, we combine all three dataset splits — **train**, **validation**, and **test** — into a single DataFrame for easier preprocessing.  

- We use **`pd.concat()`** to merge the datasets while keeping all rows.  
- After merging, we display the total number of rows, column names, and a small sample of the data.  
- We also check how many **unique labels (classes)** exist in the dataset and view the **most frequent label counts** to understand class distribution.  

This helps us confirm that the dataset is loaded correctly and gives an overview of the data structure before preprocessing.

"""

# convert (if you haven't done this already)
df_train = pd.DataFrame(ds['train'])
df_val   = pd.DataFrame(ds['validation'])
df_test  = pd.DataFrame(ds['test'])

# combine if you want a single dataframe
df = pd.concat([df_train, df_val, df_test], ignore_index=True, sort=False)

# quick info
print("Total rows:", df.shape[0])
print("Columns:", df.columns.tolist())
print("Sample:")
display(df.head())

# label stats
print("Number of unique labels:", df['label'].nunique())
print("Label value counts (top 20):")
display(df['label'].value_counts().head(20))

"""### Step 5: Checking Class Distribution

In this step, we analyze the **class distribution** in the LEDGAR dataset.

- We use **`value_counts()`** to count how many samples belong to each label (class).  
- Sorting by index helps us view the label IDs in numerical order.  
- The **`describe()`** function gives summary statistics (minimum, maximum, mean, etc.) of the class frequencies.  

This step helps identify whether the dataset is **balanced or imbalanced**, which is important for later model training and evaluation.

"""

# classes and counts
label_counts = df['label'].value_counts().sort_index()
print("Classes:", label_counts.shape[0])
label_counts.describe()   # min, max, mean of class frequencies

"""### Step 6: Saving the Combined Raw Dataset

In this step, we save the **combined LEDGAR dataset** (before preprocessing) to Google Drive.  

- We export the merged DataFrame as a **CSV file** named `LEDGAR_raw_combined.csv`.  
- This file contains all text samples and their corresponding labels from the train, validation, and test splits.  
- Saving the raw version ensures we have a **backup copy** of the original data before applying any preprocessing steps.

"""

df.to_csv('/content/drive/MyDrive/LEDGAR_raw_combined.csv', index=False)
print("Saved raw combined CSV to Drive.")

"""### Step 7: Checking Dataset Features and Metadata

In this step, we inspect the **metadata** of the LEDGAR dataset using the `features` attribute.  

- This reveals the **structure and data types** of the dataset’s fields (e.g., `text`, `label`).  
- It may also include **class label mappings** that describe what each numeric label represents (such as different legal clause types).  
- Understanding the dataset features helps ensure that we correctly interpret and preprocess each column in later steps.

"""

# Check dataset features metadata (may include class names)
print(ds['train'].features)

"""### Step 8: Text Cleaning and Preprocessing

In this step, we perform complete **text preprocessing** to clean and normalize the legal document text before using it for model training.

#### Steps performed:
1. **Convert to lowercase** – ensures uniformity across the dataset.  
2. **Expand contractions** – converts words like "can't" → "cannot" using the `contractions` library.  
3. **Remove HTML tags** – cleans any embedded tags from the text.  
4. **Remove emojis** – deletes all emojis using the `emoji` library.  
5. **Remove URLs, emails, and numbers** – ensures only meaningful words remain.  
6. **Remove special characters and punctuation** – keeps only alphabetic text.  
7. **Tokenization and Lemmatization** – splits text into tokens and converts each word to its base form using NLTK’s `WordNetLemmatizer`.  
8. **Stopword removal** – removes common words (e.g., *the, is, and*) that don’t add meaning.  
9. **Whitespace normalization** – removes extra spaces and joins tokens back into clean sentences.

After defining the `clean_text_all()` function, we apply it to every text entry in the dataset using `progress_apply()` from `tqdm` to show progress.  
The result is stored in a new column named **`clean_text`**, and a sample of original vs cleaned text is displayed for verification.

"""

# make sure stopwords/lemmatizer are ready (run earlier NLTK downloads if not)
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text_all(text):
    import re, emoji, contractions
    if not isinstance(text, str): return ""
    text = text.lower()
    text = contractions.fix(text)
    text = re.sub(r'<[^>]+>', ' ', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'http\S+|www\.\S+|\S+@\S+', ' ', text)
    text = re.sub(r'\d+', ' ', text)            # remove digits
    text = re.sub(r'[^a-z\s]', ' ', text)       # keep letters + spaces
    tokens = [lemmatizer.lemmatize(t) for t in text.split() if t not in stop_words and len(t)>1]
    return " ".join(tokens)

# apply (this will take a while for 80k rows)
from tqdm import tqdm
tqdm.pandas()
df['clean_text'] = df['text'].progress_apply(clean_text_all)

# quick check
display(df[['text','clean_text']].head())

"""### Step 9: Removing Null and Duplicate Entries

In this step, we clean the dataset further by removing any **empty or duplicate text entries** to ensure data quality.

- **`dropna(subset=['clean_text'])`** removes rows where the cleaned text is missing or null.  
- **`drop_duplicates(subset=['clean_text'])`** removes repeated legal clauses that appear more than once.  
- This helps eliminate redundant data and prevents the model from being biased or overfitting to repeated samples.

After cleaning, we print the final dataset shape to confirm the number of remaining rows and columns.

"""

# Remove nulls and duplicates
df = df.dropna(subset=['clean_text'])
df = df.drop_duplicates(subset=['clean_text'])
print("Final dataset shape:", df.shape)

"""### Step 10: Normalizing Whitespace

In this step, we perform **final text normalization** to clean up any leftover spacing issues in the dataset.

- **`str.replace(r'\s+', ' ', regex=True)`** replaces multiple spaces, tabs, or line breaks with a single space.  
- **`str.strip()`** removes any leading or trailing spaces from the text.  

This ensures that all cleaned text entries are properly formatted, making them ready for vectorization or model input.

"""

df['clean_text'] = df['clean_text'].str.replace(r'\s+', ' ', regex=True).str.strip()

"""### Step 11: Saving the Cleaned and Preprocessed Dataset

In this final step, we save the fully **cleaned and preprocessed LEDGAR dataset** to Google Drive.

- The cleaned DataFrame is exported as a CSV file named **`LEDGAR_clean_preprocessed.csv`**.  
- This file now contains only the processed text (`clean_text`) and its corresponding labels.  
- Saving the cleaned dataset allows us to easily reuse it for **model training, evaluation, or feature extraction** without repeating the preprocessing steps.

"""

output_path = '/content/drive/MyDrive/LEDGAR_clean_preprocessed.csv'
df.to_csv(output_path, index=False)
print("✅ Cleaned dataset saved to:", output_path)

"""### Step 12: Final Dataset Inspection

In this step, we perform a quick inspection of the cleaned dataset to verify that everything looks correct.

- **`df.sample(5)`** displays 5 random samples from the cleaned dataset to confirm that the text is properly processed and formatted.  
- **`df['label'].nunique()`** prints the total number of unique classes (legal clause types).  
- **`df['label'].value_counts().head()`** shows the frequency of the top 5 most common labels, helping us understand the class distribution.  

This final check ensures that the dataset is ready for further steps such as **vectorization, tokenization, or model training**.

"""

print(df.sample(5))
print("Number of classes:", df['label'].nunique())
print("Class distribution:\n", df['label'].value_counts().head())

"""### Step 13: Creating the Final Clean Dataset

In this step, we prepare the **final version** of the preprocessed dataset that will be used for model training and evaluation.

- **Step 1:** Load the previously saved cleaned dataset from Google Drive.  
- **Step 2:** Keep only the essential columns — `clean_text` (processed legal text) and `label` (class ID).  
- **Step 3:** Save this refined version as **`LEDGAR_final_clean.csv`** in Google Drive.  

This ensures that the final dataset is compact, well-structured, and ready for downstream tasks such as **vectorization, tokenization, and model training**.

"""

import pandas as pd

# 1️⃣ Load your preprocessed file
df = pd.read_csv('/content/drive/MyDrive/LEDGAR_clean_preprocessed.csv')

# 2️⃣ Keep only the clean columns
df = df[['clean_text', 'label']]

# 3️⃣ Save as the final clean dataset
df.to_csv('/content/drive/MyDrive/LEDGAR_final_clean.csv', index=False)

print("✅ Final cleaned dataset saved to: /content/drive/MyDrive/LEDGAR_final_clean.csv")
print("Shape:", df.shape)
df.head()

"""### Step 14: Viewing Label Names and Class Mapping

In this step, we retrieve and display the **label names** (class mappings) for the LEDGAR dataset.

- The LEDGAR dataset’s labels are stored as **numeric IDs**, each representing a specific **legal clause type**.  
- By accessing `ds['train'].features['label'].names`, we can view the **mapping between label IDs and their corresponding class names**.  
- This helps in interpreting model predictions later (e.g., converting numeric labels back to human-readable clause names).  
- We also print the total number of label classes and show a few examples from the mapping for reference.

"""

from datasets import load_dataset

# Load the dataset metadata only
ds = load_dataset("coastalcph/lex_glue", "ledgar")

# Get label names (mapping)
label_names = ds['train'].features['label'].names
print("Total label classes:", len(label_names))
print("Example mapping (first 10):")
for i, name in enumerate(label_names[:10]):
    print(f"{i}: {name}")

"""### Step 15: Adding Human-Readable Labels

In the original LEDGAR dataset, each document's **label** is represented as a numeric ID.  
To make the dataset easier to explore and interpret, we map these numeric IDs to their **human-readable class names** using the `label_names` list obtained earlier.

- Each numeric label is replaced with its corresponding **legal clause type name**.  
- A new column, `label_name`, is added to store these readable labels.  
- Finally, we save the updated dataset as `LEDGAR_final_clean_with_names.csv`, making it easier for both analysis and model interpretation.

"""

# Add a human-readable column
df['label_name'] = df['label'].apply(lambda x: label_names[x])

# Save the version with readable labels
df.to_csv('/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv', index=False)

print("✅ File with readable labels saved!")
df.head()

from google.colab import drive
drive.mount('/content/drive')

"""### Step 16: Chunk-Based Preprocessing Pipeline (TF-IDF, Bag of Words, One-Hot Encoding)

In this step, we perform large-scale text preprocessing on the LEDGAR dataset using a memory-efficient, chunk-based approach.  
Since the dataset is large, loading it all at once can cause memory issues. To avoid this, the processing is divided into chunks of 1000 rows, allowing transformation and saving in smaller parts.

The preprocessing includes three main operations:

---

### **1. TF-IDF Vectorization**
TF-IDF (Term Frequency–Inverse Document Frequency) converts text into numerical features that represent how important words are across documents.

**Process Overview:**
- A TF-IDF vectorizer is first fitted on a sample of 10,000 documents.
- Then, the entire dataset is processed chunk-by-chunk.
- Each chunk is transformed into TF-IDF vectors and combined with the original non-text columns.
- Intermediate files are saved every 10 chunks.
- Final output: **LEDGAR_TFIDF.csv**

This creates a consistent and memory-optimized TF-IDF feature representation for all documents.

---

### **2. Bag of Words (Count Vectorizer)**
Bag of Words creates features based on raw word counts.

**Process Overview:**
- A CountVectorizer is fitted on a sample of 10,000 texts.
- Each chunk of the dataset is transformed into word-count vectors.
- Non-text columns are preserved in the output.
- Intermediate files are saved periodically.
- Final output: **LEDGAR_BagOfWords.csv**

This representation is useful for traditional ML models that rely on frequency-based features.

---

### **3. One-Hot Encoding (Categorical Columns)**
Some non-text fields may contain categorical values (e.g., contract types, clause categories).  
These need to be converted into binary columns.

**Process Overview:**
- Categorical columns are automatically detected (excluding the main text column).
- Unique categories across the whole dataset are collected in a streaming manner.
- Each chunk is encoded into one-hot vectors using `pd.get_dummies`.
- Final output: **LEDGAR_OneHotEncoded.csv**

This ensures all non-text categorical features are properly encoded for machine learning.

---

### **Final Output Files**
All processed feature datasets are saved to Google Drive:

1. **LEDGAR_TFIDF.csv**  
2. **LEDGAR_BagOfWords.csv**  
3. **LEDGAR_OneHotEncoded.csv**

Each file is generated using chunk-based processing to avoid memory overflow.

---

### **Why This Step Is Important**
This preprocessing step converts raw legal text into structured numerical features suitable for ML tasks such as:
- Clause classification  
- Contract analysis  
- Similarity detection  
- Model training and evaluation  

By using chunked processing, this method ensures scalability even for very large datasets like LEDGAR.


"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import OneHotEncoder
import gc
import os

# File paths
input_file = '/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv'
output_dir = '/content/drive/MyDrive/'

# Configuration
CHUNK_SIZE = 1000  # Adjust based on your dataset size and memory
TEXT_COLUMN = 'clean_text'  # Your text column name

print("Starting preprocessing in chunks...")

# ============================================================================
# STEP 1: TF-IDF (Term Frequency-Inverse Document Frequency)
# ============================================================================
print("\n" + "="*70)
print("APPLYING TF-IDF")
print("="*70)

# First pass: fit the vectorizer on all data
print("Fitting TF-IDF vectorizer...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=500,  # Limit features to prevent memory issues
    min_df=2,           # Ignore terms that appear in less than 2 documents
    max_df=0.8,         # Ignore terms that appear in more than 80% of documents
    ngram_range=(1, 2)  # Use unigrams and bigrams
)

# Fit on a sample or all data (in chunks)
sample_texts = []
for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    if TEXT_COLUMN in chunk.columns:
        sample_texts.extend(chunk[TEXT_COLUMN].fillna('').astype(str).tolist())
    if len(sample_texts) >= 10000:  # Fit on first 10k documents
        break

tfidf_vectorizer.fit(sample_texts)
del sample_texts
gc.collect()

# Second pass: transform and save
print("Transforming data with TF-IDF...")
tfidf_chunks = []
chunk_num = 0

for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_num += 1
    print(f"Processing TF-IDF chunk {chunk_num}...")

    if TEXT_COLUMN in chunk.columns:
        # Transform text to TF-IDF features
        tfidf_matrix = tfidf_vectorizer.transform(chunk[TEXT_COLUMN].fillna('').astype(str))

        # Convert to DataFrame
        tfidf_df = pd.DataFrame(
            tfidf_matrix.toarray(),
            columns=[f'tfidf_{feature}' for feature in tfidf_vectorizer.get_feature_names_out()]
        )

        # Add back non-text columns
        non_text_cols = [col for col in chunk.columns if col != TEXT_COLUMN]
        for col in non_text_cols:
            tfidf_df[col] = chunk[col].values

        tfidf_chunks.append(tfidf_df)

        # Save intermediate results every 10 chunks
        if chunk_num % 10 == 0:
            temp_df = pd.concat(tfidf_chunks, ignore_index=True)
            temp_df.to_csv(f'{output_dir}tfidf_temp_{chunk_num}.csv', index=False)
            tfidf_chunks = []
            del temp_df
            gc.collect()
            print(f"  Saved intermediate file: tfidf_temp_{chunk_num}.csv")

# Save final TF-IDF results
if tfidf_chunks:
    final_tfidf = pd.concat(tfidf_chunks, ignore_index=True)
    final_tfidf.to_csv(f'{output_dir}LEDGAR_TFIDF.csv', index=False)
    print(f"✓ TF-IDF completed! Saved to: LEDGAR_TFIDF.csv")
    del final_tfidf
else:
    # Combine all temp files
    print("Combining temporary TF-IDF files...")
    temp_files = [f for f in os.listdir(output_dir) if f.startswith('tfidf_temp_')]
    combined = []
    for temp_file in sorted(temp_files):
        combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
    final_tfidf = pd.concat(combined, ignore_index=True)
    final_tfidf.to_csv(f'{output_dir}LEDGAR_TFIDF.csv', index=False)
    print(f"✓ TF-IDF completed! Saved to: LEDGAR_TFIDF.csv")

del tfidf_chunks
gc.collect()

# ============================================================================
# STEP 2: BAG OF WORDS (Count Vectorizer)
# ============================================================================
print("\n" + "="*70)
print("APPLYING BAG OF WORDS")
print("="*70)

# Fit the vectorizer
print("Fitting Bag of Words vectorizer...")
bow_vectorizer = CountVectorizer(
    max_features=500,   # Limit features
    min_df=2,
    max_df=0.8,
    ngram_range=(1, 2)
)

# Fit on sample
sample_texts = []
for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    if TEXT_COLUMN in chunk.columns:
        sample_texts.extend(chunk[TEXT_COLUMN].fillna('').astype(str).tolist())
    if len(sample_texts) >= 10000:
        break

bow_vectorizer.fit(sample_texts)
del sample_texts
gc.collect()

# Transform and save
print("Transforming data with Bag of Words...")
bow_chunks = []
chunk_num = 0

for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_num += 1
    print(f"Processing BoW chunk {chunk_num}...")

    if TEXT_COLUMN in chunk.columns:
        bow_matrix = bow_vectorizer.transform(chunk[TEXT_COLUMN].fillna('').astype(str))

        bow_df = pd.DataFrame(
            bow_matrix.toarray(),
            columns=[f'bow_{feature}' for feature in bow_vectorizer.get_feature_names_out()]
        )

        non_text_cols = [col for col in chunk.columns if col != TEXT_COLUMN]
        for col in non_text_cols:
            bow_df[col] = chunk[col].values

        bow_chunks.append(bow_df)

        if chunk_num % 10 == 0:
            temp_df = pd.concat(bow_chunks, ignore_index=True)
            temp_df.to_csv(f'{output_dir}bow_temp_{chunk_num}.csv', index=False)
            bow_chunks = []
            del temp_df
            gc.collect()
            print(f"  Saved intermediate file: bow_temp_{chunk_num}.csv")

# Save final BoW results
if bow_chunks:
    final_bow = pd.concat(bow_chunks, ignore_index=True)
    final_bow.to_csv(f'{output_dir}LEDGAR_BagOfWords.csv', index=False)
    print(f"✓ Bag of Words completed! Saved to: LEDGAR_BagOfWords.csv")
    del final_bow
else:
    print("Combining temporary BoW files...")
    temp_files = [f for f in os.listdir(output_dir) if f.startswith('bow_temp_')]
    combined = []
    for temp_file in sorted(temp_files):
        combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
    final_bow = pd.concat(combined, ignore_index=True)
    final_bow.to_csv(f'{output_dir}LEDGAR_BagOfWords.csv', index=False)
    print(f"✓ Bag of Words completed! Saved to: LEDGAR_BagOfWords.csv")

del bow_chunks
gc.collect()

# ============================================================================
# STEP 3: ONE HOT ENCODING (for categorical columns)
# ============================================================================
print("\n" + "="*70)
print("APPLYING ONE HOT ENCODING")
print("="*70)

# Identify categorical columns (adjust as needed)
first_chunk = pd.read_csv(input_file, nrows=100)
categorical_cols = first_chunk.select_dtypes(include=['object', 'category']).columns.tolist()
if TEXT_COLUMN in categorical_cols:
    categorical_cols.remove(TEXT_COLUMN)  # Don't one-hot encode text column

print(f"Categorical columns found: {categorical_cols}")

if categorical_cols:
    # Get unique categories from all data
    print("Finding unique categories...")
    unique_categories = {}
    for col in categorical_cols:
        unique_categories[col] = set()

    for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
        for col in categorical_cols:
            if col in chunk.columns:
                unique_categories[col].update(chunk[col].dropna().unique())

    # Transform and save
    print("Transforming data with One Hot Encoding...")
    ohe_chunks = []
    chunk_num = 0

    for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
        chunk_num += 1
        print(f"Processing OHE chunk {chunk_num}...")

        ohe_df = chunk.copy()

        # Apply one-hot encoding to each categorical column
        for col in categorical_cols:
            if col in chunk.columns:
                dummies = pd.get_dummies(chunk[col], prefix=f'ohe_{col}', drop_first=False)
                ohe_df = pd.concat([ohe_df, dummies], axis=1)
                ohe_df.drop(col, axis=1, inplace=True)

        ohe_chunks.append(ohe_df)

        if chunk_num % 10 == 0:
            temp_df = pd.concat(ohe_chunks, ignore_index=True)
            temp_df.to_csv(f'{output_dir}ohe_temp_{chunk_num}.csv', index=False)
            ohe_chunks = []
            del temp_df
            gc.collect()
            print(f"  Saved intermediate file: ohe_temp_{chunk_num}.csv")

    # Save final OHE results
    if ohe_chunks:
        final_ohe = pd.concat(ohe_chunks, ignore_index=True)
        final_ohe.to_csv(f'{output_dir}LEDGAR_OneHotEncoded.csv', index=False)
        print(f"✓ One Hot Encoding completed! Saved to: LEDGAR_OneHotEncoded.csv")
    else:
        print("Combining temporary OHE files...")
        temp_files = [f for f in os.listdir(output_dir) if f.startswith('ohe_temp_')]
        combined = []
        for temp_file in sorted(temp_files):
            combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
        final_ohe = pd.concat(combined, ignore_index=True)
        final_ohe.to_csv(f'{output_dir}LEDGAR_OneHotEncoded.csv', index=False)
        print(f"✓ One Hot Encoding completed! Saved to: LEDGAR_OneHotEncoded.csv")
else:
    print("⚠ No categorical columns found for One Hot Encoding")

print("\n" + "="*70)
print("ALL PROCESSING COMPLETE!")
print("="*70)
print("\nOutput files:")
print("1. LEDGAR_TFIDF.csv")
print("2. LEDGAR_BagOfWords.csv")
print("3. LEDGAR_OneHotEncoded.csv")
print("\nAll files saved to:", output_dir)

"""### Step 17: Installing Required Library (gensim)

In this step, we install the **gensim** library using the command `!pip install gensim`.  
Gensim is a powerful Python library specifically designed for natural language processing tasks that involve vector-based text representations.

It provides tools for training and using models such as **Word2Vec**, **FastText**, **Doc2Vec**, and for loading **pre-trained GloVe embeddings**.  
Since our project uses these embedding techniques to convert legal text into dense numerical vectors, installing gensim is essential before running any embedding-related code.

This command ensures that all required dependencies for creating and loading embeddings are available in the environment.

"""

!pip install gensim

"""

---

### Step 18: Applying Embedding Techniques (Word2Vec, GloVe, FastText)

In this step, we generate dense semantic vector representations for the LEDGAR dataset using three different embedding techniques: **Word2Vec**, **GloVe**, and **FastText**.  
These embeddings convert each legal clause into a fixed-length numerical vector, useful for machine learning models such as classifiers, similarity models, or clustering.

This process is done **in chunks** to avoid memory issues when working with large datasets.

---

### **1. Word2Vec Embeddings**
Word2Vec learns vector representations by analyzing the context in which words appear.  
Two training approaches can be used: CBOW and Skip-Gram (gensim chooses Skip-Gram by default for small datasets).

**How this step works:**
- Loads up to 50,000 cleaned text documents.
- Splits each sentence into tokens.
- Trains a Word2Vec model with a chosen embedding dimension (e.g., 100).
- Converts each text into an average embedding of word vectors.
- Processes the full dataset in chunks of 1000 rows.
- Saves the final output as **LEDGAR_Word2Vec.csv**.

This produces dense vectors that capture semantic similarity between legal terms.

---

### **2. GloVe Embeddings (Pre-trained)**
GloVe (*Global Vectors for Word Representation*) is a pre-trained embedding model trained on large corpora like Wikipedia or Twitter.

**Process Summary:**
- Downloads a pre-trained GloVe model (`glove-wiki-gigaword-100`).  
- If unavailable, automatically switches to another model (e.g., `glove-twitter-25`).
- Converts each clause into an embedding by averaging the vectors of its words.
- Processes the full dataset in memory-safe chunks.
- Saves the final output as **LEDGAR_GloVe.csv**.

Since GloVe is pre-trained on huge corpora, it often provides richer embeddings compared to training from scratch.

---

### **3. FastText Embeddings**
FastText improves on Word2Vec by representing words as combinations of character n-grams.  
This allows it to generate embeddings for *out-of-vocabulary (OOV)* or rare words — helpful in legal datasets.

**Process Overview:**
- Loads up to 50,000 LEDGAR sentences.
- Trains a FastText model (subword-based embedding).
- Produces embeddings for each clause via averaging.
- Uses chunk-based processing to prevent memory overflow.
- Saves output as **LEDGAR_FastText.csv**.

FastText is especially useful for domain-specific and complex legal terminology.

---

### **Output Files Generated**
After processing all chunks, three embedding feature files are saved:

1. **LEDGAR_Word2Vec.csv**  
2. **LEDGAR_GloVe.csv**  
3. **LEDGAR_FastText.csv**

Each file contains:
- A fixed number of embedding dimensions (e.g., 100)
- Original metadata columns (e.g., label, label_name)
- Chunk-processed dense vector representations of the text

---

### **Purpose of This Step**
These embeddings transform each legal clause into a numerical vector capturing:
- Semantic meaning  
- Contextual relationships  
- Similarity between clause types  

These representations are essential for downstream ML tasks such as:
- Clause classification  
- Contract analysis  
- Clustering or similarity-based retrieval  
- Neural network training  

This ensures the dataset is converted into powerful, model-ready numerical features while maintaining efficient memory usage.

"""

import pandas as pd
import numpy as np
from gensim.models import Word2Vec, FastText
from gensim.models import KeyedVectors
import gensim.downloader as api
import gc
import warnings
warnings.filterwarnings('ignore')

# File paths
input_file = '/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv'
output_dir = '/content/drive/MyDrive/'

# Configuration
CHUNK_SIZE = 1000  # Adjust based on memory
TEXT_COLUMN = 'clean_text'
EMBEDDING_DIM = 100  # Dimension of embeddings (can change to 200, 300)

print("Starting Embedding Techniques...")
print("="*70)

# ============================================================================
# HELPER FUNCTION: Convert text to embedding vector
# ============================================================================
def text_to_embedding(text, model, embedding_dim):
    """Convert text to average embedding vector"""
    words = str(text).lower().split()
    word_vectors = []

    for word in words:
        try:
            if hasattr(model, 'wv'):  # Word2Vec/FastText
                word_vectors.append(model.wv[word])
            else:  # GloVe
                word_vectors.append(model[word])
        except KeyError:
            continue

    if len(word_vectors) > 0:
        return np.mean(word_vectors, axis=0)
    else:
        return np.zeros(embedding_dim)

# ============================================================================
# STEP 1: WORD2VEC
# ============================================================================
print("\n" + "="*70)
print("APPLYING WORD2VEC")
print("="*70)

# Prepare sentences for training
print("Loading data for Word2Vec training...")
sentences = []
for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_sentences = chunk[TEXT_COLUMN].fillna('').astype(str).apply(lambda x: x.lower().split()).tolist()
    sentences.extend(chunk_sentences)
    if len(sentences) >= 50000:  # Limit training data to prevent memory issues
        break

print(f"Training Word2Vec model on {len(sentences)} documents...")
w2v_model = Word2Vec(
    sentences=sentences,
    vector_size=EMBEDDING_DIM,
    window=5,
    min_count=2,
    workers=4,
    epochs=10
)
print("✓ Word2Vec model trained!")

del sentences
gc.collect()

# Transform data and save
print("Transforming data with Word2Vec...")
w2v_chunks = []
chunk_num = 0

for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_num += 1
    print(f"Processing Word2Vec chunk {chunk_num}...")

    # Get embeddings for each text
    embeddings = chunk[TEXT_COLUMN].fillna('').astype(str).apply(
        lambda x: text_to_embedding(x, w2v_model, EMBEDDING_DIM)
    )

    # Convert to DataFrame
    embedding_df = pd.DataFrame(
        embeddings.tolist(),
        columns=[f'w2v_dim_{i}' for i in range(EMBEDDING_DIM)]
    )

    # Add original non-text columns
    non_text_cols = [col for col in chunk.columns if col != TEXT_COLUMN]
    for col in non_text_cols:
        embedding_df[col] = chunk[col].values

    w2v_chunks.append(embedding_df)

    # Save intermediate results every 10 chunks
    if chunk_num % 10 == 0:
        temp_df = pd.concat(w2v_chunks, ignore_index=True)
        temp_df.to_csv(f'{output_dir}w2v_temp_{chunk_num}.csv', index=False)
        w2v_chunks = []
        del temp_df
        gc.collect()
        print(f"  Saved intermediate file: w2v_temp_{chunk_num}.csv")

# Save final Word2Vec results
if w2v_chunks:
    final_w2v = pd.concat(w2v_chunks, ignore_index=True)
    final_w2v.to_csv(f'{output_dir}LEDGAR_Word2Vec.csv', index=False)
    print(f"✓ Word2Vec completed! Saved to: LEDGAR_Word2Vec.csv")
    del final_w2v
else:
    print("Combining temporary Word2Vec files...")
    import os
    temp_files = sorted([f for f in os.listdir(output_dir) if f.startswith('w2v_temp_')])
    combined = []
    for temp_file in temp_files:
        combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
        os.remove(os.path.join(output_dir, temp_file))  # Clean up temp files
    final_w2v = pd.concat(combined, ignore_index=True)
    final_w2v.to_csv(f'{output_dir}LEDGAR_Word2Vec.csv', index=False)
    print(f"✓ Word2Vec completed! Saved to: LEDGAR_Word2Vec.csv")

del w2v_chunks, w2v_model
gc.collect()

# ============================================================================
# STEP 2: GLOVE (Using pre-trained GloVe)
# ============================================================================
print("\n" + "="*70)
print("APPLYING GLOVE")
print("="*70)

print("Downloading pre-trained GloVe model (this may take a few minutes)...")
try:
    # Download GloVe embeddings (100 dimensions)
    glove_model = api.load("glove-wiki-gigaword-100")
    print("✓ GloVe model loaded!")
except Exception as e:
    print(f"Error loading GloVe: {e}")
    print("Trying alternative method...")
    glove_model = api.load("glove-twitter-25")  # Smaller model as backup

# Transform data and save
print("Transforming data with GloVe...")
glove_chunks = []
chunk_num = 0

for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_num += 1
    print(f"Processing GloVe chunk {chunk_num}...")

    # Get embeddings for each text
    embeddings = chunk[TEXT_COLUMN].fillna('').astype(str).apply(
        lambda x: text_to_embedding(x, glove_model, EMBEDDING_DIM)
    )

    # Convert to DataFrame
    embedding_df = pd.DataFrame(
        embeddings.tolist(),
        columns=[f'glove_dim_{i}' for i in range(EMBEDDING_DIM)]
    )

    # Add original non-text columns
    non_text_cols = [col for col in chunk.columns if col != TEXT_COLUMN]
    for col in non_text_cols:
        embedding_df[col] = chunk[col].values

    glove_chunks.append(embedding_df)

    if chunk_num % 10 == 0:
        temp_df = pd.concat(glove_chunks, ignore_index=True)
        temp_df.to_csv(f'{output_dir}glove_temp_{chunk_num}.csv', index=False)
        glove_chunks = []
        del temp_df
        gc.collect()
        print(f"  Saved intermediate file: glove_temp_{chunk_num}.csv")

# Save final GloVe results
if glove_chunks:
    final_glove = pd.concat(glove_chunks, ignore_index=True)
    final_glove.to_csv(f'{output_dir}LEDGAR_GloVe.csv', index=False)
    print(f"✓ GloVe completed! Saved to: LEDGAR_GloVe.csv")
    del final_glove
else:
    print("Combining temporary GloVe files...")
    temp_files = sorted([f for f in os.listdir(output_dir) if f.startswith('glove_temp_')])
    combined = []
    for temp_file in temp_files:
        combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
        os.remove(os.path.join(output_dir, temp_file))
    final_glove = pd.concat(combined, ignore_index=True)
    final_glove.to_csv(f'{output_dir}LEDGAR_GloVe.csv', index=False)
    print(f"✓ GloVe completed! Saved to: LEDGAR_GloVe.csv")

del glove_chunks, glove_model
gc.collect()

# ============================================================================
# STEP 3: FASTTEXT
# ============================================================================
print("\n" + "="*70)
print("APPLYING FASTTEXT")
print("="*70)

# Prepare sentences for training
print("Loading data for FastText training...")
sentences = []
for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_sentences = chunk[TEXT_COLUMN].fillna('').astype(str).apply(lambda x: x.lower().split()).tolist()
    sentences.extend(chunk_sentences)
    if len(sentences) >= 50000:
        break

print(f"Training FastText model on {len(sentences)} documents...")
ft_model = FastText(
    sentences=sentences,
    vector_size=EMBEDDING_DIM,
    window=5,
    min_count=2,
    workers=4,
    epochs=10
)
print("✓ FastText model trained!")

del sentences
gc.collect()

# Transform data and save
print("Transforming data with FastText...")
ft_chunks = []
chunk_num = 0

for chunk in pd.read_csv(input_file, chunksize=CHUNK_SIZE):
    chunk_num += 1
    print(f"Processing FastText chunk {chunk_num}...")

    # Get embeddings for each text
    embeddings = chunk[TEXT_COLUMN].fillna('').astype(str).apply(
        lambda x: text_to_embedding(x, ft_model, EMBEDDING_DIM)
    )

    # Convert to DataFrame
    embedding_df = pd.DataFrame(
        embeddings.tolist(),
        columns=[f'fasttext_dim_{i}' for i in range(EMBEDDING_DIM)]
    )

    # Add original non-text columns
    non_text_cols = [col for col in chunk.columns if col != TEXT_COLUMN]
    for col in non_text_cols:
        embedding_df[col] = chunk[col].values

    ft_chunks.append(embedding_df)

    if chunk_num % 10 == 0:
        temp_df = pd.concat(ft_chunks, ignore_index=True)
        temp_df.to_csv(f'{output_dir}ft_temp_{chunk_num}.csv', index=False)
        ft_chunks = []
        del temp_df
        gc.collect()
        print(f"  Saved intermediate file: ft_temp_{chunk_num}.csv")

# Save final FastText results
if ft_chunks:
    final_ft = pd.concat(ft_chunks, ignore_index=True)
    final_ft.to_csv(f'{output_dir}LEDGAR_FastText.csv', index=False)
    print(f"✓ FastText completed! Saved to: LEDGAR_FastText.csv")
    del final_ft
else:
    print("Combining temporary FastText files...")
    temp_files = sorted([f for f in os.listdir(output_dir) if f.startswith('ft_temp_')])
    combined = []
    for temp_file in temp_files:
        combined.append(pd.read_csv(os.path.join(output_dir, temp_file)))
        os.remove(os.path.join(output_dir, temp_file))
    final_ft = pd.concat(combined, ignore_index=True)
    final_ft.to_csv(f'{output_dir}LEDGAR_FastText.csv', index=False)
    print(f"✓ FastText completed! Saved to: LEDGAR_FastText.csv")

del ft_chunks, ft_model
gc.collect()

print("\n" + "="*70)
print("ALL EMBEDDING TECHNIQUES COMPLETE!")
print("="*70)
print("\nOutput files created:")
print("1. LEDGAR_Word2Vec.csv")
print("2. LEDGAR_GloVe.csv")
print("3. LEDGAR_FastText.csv")
print("\nAll files saved to:", output_dir)
print("\nEach file contains:")
print(f"  - {EMBEDDING_DIM} embedding dimensions")
print("  - Original 'label' and 'label_name' columns")
print("="*70)

"""# Task 6: Model Definition - Legal-BERT for Legal Document Classification

---

## 1. Overview

**Task:** Multi-class Classification of Legal Document Types  
**Dataset:** LEDGAR (Legal Document Classification Dataset)  
**Model:** Legal-BERT (Legal Bidirectional Encoder Representations from Transformers)  
**Purpose:** Automatically classify legal documents into predefined categories based on their textual content.

---

## 2. What is Legal-BERT?

Legal-BERT is a specialized variant of BERT (Bidirectional Encoder Representations from Transformers) that has been pre-trained specifically on legal text corpora. Unlike the original BERT trained on general text (Wikipedia, BookCorpus), Legal-BERT is trained on:
- Legal cases
- Contracts
- Legislation documents
- Legal opinions

This domain-specific pre-training allows Legal-BERT to better understand legal terminology, phrases, and document structures.

---

## 3. Model Architecture

Legal-BERT follows the transformer architecture introduced by Vaswani et al. (2017), specifically using the BERT-base configuration with 12 layers, 12 attention heads, and 768 hidden dimensions. The architecture can be divided into three main components: the input processing layer, the transformer encoder stack, and the classification head.

### High-Level Architecture Flow

```
Input: Legal Document Text
         │
         ▼
┌─────────────────────┐
│  WordPiece Tokenize │
└─────────────────────┘
         │
         ▼
┌─────────────────────┐
│ Add Special Tokens  │
│  [CLS] ... [SEP]    │
└─────────────────────┘
         │
         ▼
┌───────────────────────────────────┐
│      EMBEDDING LAYER (768d)       │
├───────────────────────────────────┤
│  • Token Embeddings (30522 → 768) │
│  • Position Embeddings (512 → 768)│
│  • Segment Embeddings (2 → 768)   │
└───────────────────────────────────┘
         │
         ▼
┌───────────────────────────────────┐
│   12 TRANSFORMER ENCODER LAYERS   │
├───────────────────────────────────┤
│  Each Layer:                      │
│  • Multi-Head Self-Attention (12) │
│  • Feed-Forward Network           │
│  • Layer Normalization            │
│  • Residual Connections           │
└───────────────────────────────────┘
         │
         ▼
┌─────────────────────┐
│ Extract [CLS] Token │
│     (768 dim)       │
└─────────────────────┘
         │
         ▼
┌─────────────────────┐
│  Classification Head│
│  • Dropout (p=0.1)  │
│  • Linear (768→100) │
│  • Softmax          │
└─────────────────────┘
         │
         ▼
Output: Predicted Class
```

### Detailed Transformer Layer Structure

```
╔═══════════════════════════════════════════════════════════╗
║              SINGLE TRANSFORMER LAYER                      ║
╚═══════════════════════════════════════════════════════════╝

Input: [batch_size, 512, 768]
    │
    ├─────────────────────────────────────────┐
    │                                          │
    │   ┌──────────────────────────────────┐  │
    │   │  MULTI-HEAD SELF-ATTENTION       │  │
    │   ├──────────────────────────────────┤  │
    │   │  Query (Q) = Input × W_Q         │  │
    │   │  Key (K)   = Input × W_K         │  │
    │   │  Value (V) = Input × W_V         │  │
    │   │                                  │  │
    │   │  Split into 12 heads             │  │
    │   │  Each head: 64 dimensions        │  │
    │   │                                  │  │
    │   │  Attention = softmax(QK^T/√64)·V │  │
    │   │                                  │  │
    │   │  Concatenate all 12 heads        │  │
    │   └──────────────────────────────────┘  │
    │                 │                        │
    │                 ▼                        │
    │   ┌──────────────────────────────────┐  │
    │   │      Add & Layer Normalize       │◀─┘
    │   └──────────────────────────────────┘
    │                 │
    │                 ▼
    ├─────────────────────────────────────────┐
    │                                          │
    │   ┌──────────────────────────────────┐  │
    │   │    FEED-FORWARD NETWORK          │  │
    │   ├──────────────────────────────────┤  │
    │   │  Linear1: 768 → 3072             │  │
    │   │  GELU Activation                 │  │
    │   │  Linear2: 3072 → 768             │  │
    │   └──────────────────────────────────┘  │
    │                 │                        │
    │                 ▼                        │
    │   ┌──────────────────────────────────┐  │
    │   │      Add & Layer Normalize       │◀─┘
    │   └──────────────────────────────────┘
    │                 │
    ▼                 ▼
Output: [batch_size, 512, 768]
```

### Core Components

#### a) Input Layer
| Component | Details |
|-----------|---------|
| Tokenizer | WordPiece with legal vocabulary (30,522 tokens) |
| Max Length | 512 tokens |
| Special Tokens | `[CLS]` (start), `[SEP]` (end), `[PAD]` (padding) |

#### b) Embedding Layer
Three embeddings combined element-wise:
- **Token Embeddings (768d):** Represent word meaning
- **Position Embeddings (768d):** Encode token position
- **Segment Embeddings (768d):** Distinguish text segments

The embedding layer is the first component that processes the input tokens. It converts discrete token IDs into continuous vector representations that the model can process. The three types of embeddings serve different purposes:

**Token Embeddings:** These are learned representations for each of the 30,522 tokens in the vocabulary. Each token is mapped to a 768-dimensional vector. For example, the word "agreement" will have a unique embedding vector that captures its semantic meaning based on how it appears in legal contexts during pre-training.

**Position Embeddings:** Since the Transformer architecture doesn't inherently understand the order of tokens (unlike RNNs), position embeddings are added to give the model information about where each token appears in the sequence. Legal-BERT uses learned position embeddings for positions 0 through 511.

**Segment Embeddings:** These embeddings help distinguish between different segments of text. In our single-document classification task, all tokens belong to the same segment, so all segment embeddings will be identical. However, this feature is useful in tasks like question-answering where you have pairs of sentences.

The final embedding for each token is the element-wise sum of these three embeddings:
```
Final_Embedding = Token_Emb + Position_Emb + Segment_Emb
```

#### c) Transformer Encoder (12 Layers)
Each layer contains:
- **Multi-Head Self-Attention (12 heads):** Captures relationships between all tokens
- **Feed-Forward Network:** Two linear layers with GELU activation (768 → 3072 → 768)
- **Layer Normalization:** Applied after each sub-layer
- **Residual Connections:** Help with gradient flow

The transformer encoder is the core of Legal-BERT, consisting of 12 identical layers stacked on top of each other. Each layer progressively refines the representation of the input.

**Multi-Head Self-Attention Mechanism:**
This is the key innovation of the Transformer architecture. Instead of processing tokens sequentially, self-attention allows each token to "look at" all other tokens in the sequence simultaneously and determine which tokens are most relevant to understanding its meaning.

For example, in the sentence "The party shall indemnify the other party," when processing the second occurrence of "party," the attention mechanism will strongly attend to "other" to understand that this refers to a different party than the first mention.

The attention is computed as:
```
Attention(Q, K, V) = softmax(Q·K^T / √d_k) · V
```
Where Q (Query), K (Key), and V (Value) are different linear projections of the input.

**Multi-Head Attention:** Rather than computing attention once, Legal-BERT uses 12 attention heads in parallel. Each head can focus on different aspects of the relationships between tokens. For instance, one head might focus on syntactic relationships (subject-verb), while another focuses on semantic relationships (entity-definition). The outputs of all 12 heads are concatenated and projected back to 768 dimensions.

**Feed-Forward Network:**
After the attention layer, each token representation passes through a position-wise feed-forward network. This consists of two linear transformations with a GELU activation in between:
```
FFN(x) = GELU(x·W1 + b1)·W2 + b2
```
The intermediate dimension is 3072 (4 times the hidden size), which allows the model to learn complex transformations.

**Residual Connections and Layer Normalization:**
After both the attention and feed-forward sub-layers, residual connections add the input of the sub-layer to its output. This helps with training deep networks by allowing gradients to flow more easily. Layer normalization then stabilizes the training process.

#### d) Classification Head
- Extract [CLS] token representation (768d)
- Apply Dropout (p=0.1)
- Linear layer: 768 → 100 classes
- Softmax activation for probabilities

The classification head is task-specific and added on top of the pre-trained Legal-BERT model. During pre-training, Legal-BERT learns general language understanding, but the classification head is what specializes it for our specific task of classifying legal provision types.

**[CLS] Token:** The special [CLS] token is always placed at the beginning of the input sequence. After passing through all 12 transformer layers, the final hidden state of this token serves as an aggregate representation of the entire input sequence. Think of it as a "summary vector" that encodes information about the whole document.

**Dropout:** A dropout layer with probability 0.1 is applied to prevent overfitting. During training, 10% of the neurons are randomly "dropped out" (set to zero), which forces the network to learn more robust features that don't rely on specific neurons.

**Linear Layer:** This fully connected layer projects the 768-dimensional [CLS] representation to 100 dimensions (one for each provision type in LEDGAR). The weights of this layer are randomly initialized and learned during fine-tuning.

**Softmax:** Finally, the softmax function converts the raw scores (logits) into a probability distribution over the 100 classes, where all probabilities sum to 1.0.

---

## 4. How Legal-BERT Processes Input (Step-by-Step)

**Step 1: Text Preprocessing**
```
Raw Input: "This Agreement shall be governed by the laws of Delaware."
Tokenized: ["[CLS]", "This", "Agreement", "shall", "be", "governed",
            "by", "the", "laws", "of", "Delaware", ".", "[SEP]"]
```

**Step 2: Convert to IDs**
```
Token IDs:      [101, 2023, 8567, 4099, 2022, 8900, ..., 102]
Attention Mask: [1, 1, 1, 1, 1, 1, ..., 1]
```

**Step 3: Generate Embeddings**
- Each token → 768-dimensional vector
- Position encoding added
- Combined: [sequence_length × 768]

**Step 4: Pass Through 12 Transformer Layers**
- Each layer performs self-attention and feed-forward transformations
- Token "Agreement" attends to context words like "governed", "laws"
- Progressive refinement of representations

**Step 5: Extract [CLS] Token**
- After 12 layers, [CLS] token contains document representation
- Shape: [768]

**Step 6: Classification**
```
[CLS] [768] → Dropout → Linear [100] → Softmax
Output Probabilities: [0.05, 0.82, 0.03, ..., 0.01]
Predicted Class: 1 (highest probability = 0.82)
```

---

## 5. Model Parameters

| Component | Value | Description |
|-----------|-------|-------------|
| Model Type | BERT-Base | Base size architecture |
| Vocabulary Size | 30,522 | Legal domain vocabulary |
| Hidden Size | 768 | Dimension of embeddings |
| Num Layers | 12 | Transformer encoder layers |
| Attention Heads | 12 | Multi-head attention |
| Intermediate Size | 3,072 | Feed-forward hidden dimension |
| Max Sequence Length | 512 | Maximum tokens |
| Dropout | 0.1 | Dropout probability |
| **Total Parameters** | **~110M** | **Trainable parameters** |

---

## 6. Training Process

### Loss Function
**Cross-Entropy Loss:**
```
Loss = -Σ(y_true · log(y_pred))
```

Cross-entropy loss is the standard loss function for multi-class classification problems. It measures the difference between the predicted probability distribution and the true distribution (one-hot encoded label).

For example, if the true label is class 5 (one-hot: [0,0,0,0,1,0,...,0]) and the model predicts probabilities [0.01, 0.02, 0.05, 0.10, 0.75, 0.03, ..., 0.01], the loss will be -log(0.75) ≈ 0.29. If the model was more confident and predicted 0.95 for the correct class, the loss would be -log(0.95) ≈ 0.05 (lower is better).

The loss is high when the model is uncertain or wrong, and low when the model is confident and correct. During training, we minimize this loss by updating the model's parameters.

### Training Configuration

| Hyperparameter | Value | Explanation |
|----------------|-------|-------------|
| Optimizer | AdamW | Adam with weight decay for better generalization |
| Learning Rate | 2e-5 | Small LR prevents destroying pre-trained weights |
| Batch Size | 16-32 | Balance between speed and memory |
| Epochs | 3-5 | Prevent overfitting on training data |
| Weight Decay | 0.01 | L2 regularization to reduce overfitting |
| Warmup Steps | 500 | Gradually increase LR at the start |
| Max Grad Norm | 1.0 | Clip gradients to prevent exploding gradients |

### Fine-tuning Strategy
Fine-tuning is the process of adapting a pre-trained model to a specific task. Unlike training from scratch, we start with Legal-BERT's pre-trained weights, which already understand legal language, and make small adjustments to specialize it for provision classification.

**The fine-tuning process:**
1. **Load pre-trained Legal-BERT weights:** These weights were learned from processing millions of legal documents
2. **Add randomly initialized classification head:** The 768→100 linear layer starts with random weights
3. **Fine-tune all parameters on LEDGAR dataset:** Both the BERT layers and classification head are updated, but with a small learning rate
4. **Use early stopping based on validation F1:** Stop training if validation performance stops improving to prevent overfitting

**Why use a small learning rate (2e-5)?**
The pre-trained weights already contain valuable legal knowledge. A large learning rate could destroy this knowledge. A small learning rate makes gentle adjustments to adapt the model to our specific task.

**Warmup steps:**
Instead of starting at the full learning rate immediately, we gradually increase it over the first 500 steps. This helps stabilize training at the beginning.

---

## 7. Model Implementation (Code Snippet)

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Configuration
MODEL_NAME = "nlpaueb/legal-bert-base-uncased"
NUM_CLASSES = 100  # LEDGAR has 100 provision types

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_CLASSES,
    problem_type="single_label_classification"
)

print(f"Total Parameters: {model.num_parameters():,}")

# Example inference
text = "This Agreement shall be binding upon the parties."
inputs = tokenizer(text, return_tensors='pt', max_length=512,
                   truncation=True, padding='max_length')

outputs = model(**inputs)
predictions = outputs.logits.argmax(-1)
```

---

## 8. Evaluation Metrics

Understanding how well our model performs requires multiple metrics, as each captures different aspects of performance.

### 1. Accuracy
**Formula:** `Accuracy = (TP + TN) / Total Predictions`

**Purpose:** Overall correctness percentage

Accuracy tells us what percentage of predictions are correct. While it's intuitive and easy to understand, it can be misleading when classes are imbalanced. For example, if 90% of documents belong to one class, a model that always predicts that class would have 90% accuracy but would be useless in practice.

**Example:**
- Total predictions: 10,000
- Correct predictions: 8,200
- Accuracy: 82%

### 2. Macro-F1 Score
**Formula:** `Macro-F1 = (1/n) × Σ F1_i`

**Purpose:**
- Average F1 across all 100 classes
- Treats all classes equally (important for imbalanced data)
- Ensures model doesn't just predict common classes

Macro-F1 is crucial for our task because the LEDGAR dataset likely has imbalanced classes (some provision types are more common than others). By averaging the F1 score across all classes equally, macro-F1 ensures that performance on rare classes is weighted the same as performance on common classes.

**Why is this important?**
Imagine we have:
- Class A (common): 5,000 examples, F1 = 0.95
- Class B (rare): 50 examples, F1 = 0.30

A model that ignores Class B might still have high accuracy, but macro-F1 would be only (0.95 + 0.30)/2 = 0.625, revealing the problem.

**Calculation Process:**
1. Calculate Precision for each class: `P = TP / (TP + FP)`
2. Calculate Recall for each class: `R = TP / (TP + FN)`
3. Calculate F1 for each class: `F1 = 2 × (P × R) / (P + R)`
4. Average all F1 scores: `Macro-F1 = mean(F1_1, F1_2, ..., F1_100)`

### 3. Per-Class F1 Score
**Formula:** `F1 = 2 × (Precision × Recall) / (Precision + Recall)`

**Purpose:**
- Identify which provision types are harder to classify
- Monitor performance on individual classes
- Guide targeted improvements

Per-class F1 scores help us understand where the model struggles. By examining individual class performance, we can:
- Identify confusing class pairs (e.g., "Amendments" vs "Modifications")
- Discover classes that need more training data
- Find classes that might need feature engineering or data augmentation

**Example Report:**
```
┌─────────────────────┬───────────┬────────┬──────────┬─────────┐
│ Provision Type      │ Precision │ Recall │ F1-Score │ Support │
├─────────────────────┼───────────┼────────┼──────────┼─────────┤
│ Affiliates          │   0.89    │  0.92  │   0.90   │   450   │
│ Agreements          │   0.95    │  0.93  │   0.94   │   780   │
│ Amendments          │   0.78    │  0.74  │   0.76   │   230   │
│ Applicable Law      │   0.92    │  0.89  │   0.90   │   520   │
│ ...                 │   ...     │  ...   │   ...    │   ...   │
└─────────────────────┴───────────┴────────┴──────────┴─────────┘

Interpretation:
• High F1 (>0.90): Model performs excellently
• Medium F1 (0.70-0.90): Acceptable performance  
• Low F1 (<0.70): Needs improvement - consider:
  - Collecting more training examples
  - Data augmentation techniques
  - Feature engineering
  - Examining confusion with similar classes
```

**Understanding Precision vs Recall:**
- **High Precision, Low Recall:** Model is conservative - when it predicts a class, it's usually right, but it misses many examples
- **Low Precision, High Recall:** Model is aggressive - it catches most examples but makes many false positive errors
- **Balanced F1:** Harmonic mean that requires both precision and recall to be good

---

## 9. Advantages of Legal-BERT

Legal-BERT offers several significant advantages over traditional machine learning approaches and general-purpose language models for legal document classification:

1. **Domain Expertise:** Pre-trained on 12GB of legal text including contracts, case law, and legislation. This means it already understands legal terminology like "indemnification," "whereas," "force majeure," and "jurisdiction" in their proper legal context. A general BERT model might confuse "party" (legal entity) with "party" (celebration), but Legal-BERT knows the difference from context.

2. **Contextual Understanding:** Unlike traditional bag-of-words or TF-IDF approaches that treat words independently, Legal-BERT's bidirectional attention mechanism captures full context. It understands that "the party shall not" has opposite meaning to "the party shall," which is crucial for legal text where negations and modal verbs dramatically change meaning.

3. **Transfer Learning Efficiency:** By starting with pre-trained weights, Legal-BERT requires significantly less labeled training data than training from scratch. While training a transformer from scratch might require millions of labeled examples, we can fine-tune Legal-BERT effectively with tens of thousands of examples.

4. **Handling Ambiguity:** Legal documents are full of ambiguous terms whose meaning depends on context. Legal-BERT can disambiguate terms like "term" (contract duration vs. terminology) or "provisions" (contract clauses vs. supplies) based on surrounding words.

5. **Long-range Dependencies:** The self-attention mechanism allows Legal-BERT to capture relationships between words that are far apart in the text. For example, it can connect a pronoun like "it" or "such party" to its antecedent even if they're separated by many words.

6. **Proven Performance:** Legal-BERT has demonstrated state-of-the-art results on multiple legal NLP benchmarks, consistently outperforming general BERT by 5-10% on legal tasks and traditional ML methods by even larger margins.

---

## 10. Key Challenges & Solutions

### Challenge 1: Long Documents
**Problem:** Legal documents often exceed 512 tokens, but Legal-BERT can only process 512 tokens at once. A typical contract might be thousands of tokens long, meaning we lose information by truncation.

**Solutions:**

**Approach 1 - Simple Truncation:**
- Take the first 512 tokens only
- **Pros:** Simple, fast, works if important information is at the beginning
- **Cons:** Loses potentially important information from later sections
- **Use case:** When provision types are typically identified early in documents

**Approach 2 - Sliding Window:**
- Split document into overlapping chunks of 512 tokens
- Process each chunk separately
- Aggregate predictions (e.g., by voting or averaging probabilities)
- **Pros:** Uses entire document, more information preserved
- **Cons:** Slower inference (multiple forward passes), more complex implementation
- **Use case:** When important information might appear anywhere

**Approach 3 - Hierarchical Processing:**
- Use Legal-BERT to encode chunks
- Use another model to aggregate chunk representations
- **Pros:** Can handle very long documents effectively
- **Cons:** Requires more complex architecture and training

**Approach 4 - Switch to Longformer:**
- Use Longformer-based legal model that handles up to 4096 tokens
- **Pros:** Process longer sequences in one pass
- **Cons:** Requires more computational resources, fewer pre-trained legal versions available

### Challenge 2: Class Imbalance
**Problem:** In legal datasets, some provision types appear much more frequently than others. For example, "Governing Law" clauses might appear in 80% of contracts, while "Waiver of Jury Trial" might appear in only 2%.

**Impact:** The model becomes biased toward predicting common classes, achieving high overall accuracy while performing poorly on rare but important classes.

**Solutions:**

**Solution 1 - Weighted Loss Function:**
```python
class_weights = compute_class_weight('balanced',
                                     classes=np.unique(y_train),
                                     y=y_train)
```
Assign higher loss penalties to misclassifications of rare classes, forcing the model to pay more attention to them.

**Solution 2 - Oversampling:**
- Randomly duplicate examples from rare classes
- Use SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples
- **Caution:** Risk of overfitting to the oversampled examples

**Solution 3 - Data Augmentation:**
- Paraphrase rare class examples using synonym replacement
- Back-translation (translate to another language and back)
- Use large language models to generate similar examples

**Solution 4 - Focal Loss:**
Instead of standard cross-entropy, use focal loss that focuses more on hard-to-classify examples:
```
FL(pt) = -(1-pt)^γ × log(pt)
```
This down-weights easy examples and focuses on difficult ones.

**Solution 5 - Ensemble Methods:**
Train separate models on balanced subsets and combine their predictions.

### Challenge 3: Computational Cost
**Requirements:**

| Resource | Training | Inference |
|----------|----------|-----------|
| **GPU Memory** | 8-16 GB | 2-4 GB |
| **Training Time** | 2-4 hours (full dataset) | - |
| **Inference Time** | - | 10-50 ms/document |
| **Batch Size** | 16-32 | 64-128 |
| **Power Consumption** | High (GPU intensive) | Medium |

**Optimization Strategies:**

**For Training:**
- **Mixed Precision (FP16):** Use 16-bit floats instead of 32-bit, reducing memory usage by ~50% and speeding up training by ~2x
- **Gradient Accumulation:** Simulate larger batch sizes by accumulating gradients over multiple small batches
- **Distributed Training:** Split training across multiple GPUs for faster training
- **Efficient Attention:** Use optimized attention implementations like Flash Attention

**For Inference:**
- **Model Distillation:** Train a smaller "student" model to mimic Legal-BERT's behavior (e.g., DistilBERT with 6 layers instead of 12)
- **Quantization:** Convert weights from FP32 to INT8, reducing model size by 4x with minimal accuracy loss
- **ONNX Runtime:** Export model to ONNX format for optimized inference
- **Batch Inference:** Process multiple documents simultaneously to maximize GPU utilization

**Cost-Benefit Analysis:**
While Legal-BERT requires significant computational resources, the benefits often justify the cost:
- Superior accuracy compared to traditional methods
- Reduced need for manual feature engineering
- Better generalization to new types of legal documents
- Automated processing saves human review time

---

## 11. Summary

Legal-BERT is a transformer-based model specifically designed for legal text understanding. It processes legal documents through:
1. **Tokenization** with legal vocabulary
2. **Embedding layer** combining token, position, and segment information
3. **12 transformer layers** with self-attention and feed-forward networks
4. **Classification head** for multi-class prediction

The model's pre-training on legal corpora makes it highly effective for legal document classification, achieving superior performance compared to general-purpose language models while being efficient to fine-tune on domain-specific datasets like LEDGAR.

---

## References

1. Chalkidis, I., et al. (2020). "LEGAL-BERT: The Muppets straight out of Law School"
2. Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers"
3. Vaswani, A., et al. (2017). "Attention is All You Need"
4. LEDGAR Dataset: https://huggingface.co/datasets/lex_glue

gpt -> give me at least 5 graphs according to my project (mean score, co relation, cosine similarity) training and validation loss, accuracy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

print("LEDGAR Project - Comprehensive Visualizations (UPDATED WITH REAL DATA)")
print("="*70)

# ============================================================================
# ACTUAL DATA FROM YOUR TRAINING
# ============================================================================

# Training configuration
epochs = 3
train_samples = 36000
val_samples = 4000
test_samples = 10000
num_classes = 100

# Actual training metrics from your logs
train_loss = [1.6688, 0.6306, 0.4514]
train_acc = [0.6541, 0.8346, 0.8770]
val_loss = [0.7796, 0.6550, 0.6239]
val_acc = [0.8025, 0.8327, 0.8385]

# Test metrics
test_accuracy = 0.8400
test_precision = 0.7659
test_recall = 0.7426
test_f1 = 0.7415

# Per-class metrics from classification report
class_ids = [0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26,
             27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 43, 44,
             45, 46, 47, 48, 49, 5, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 6, 60, 61, 62,
             63, 64, 65, 66, 67, 68, 69, 7, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 8, 80,
             81, 82, 83, 84, 85, 86, 87, 88, 89, 9, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]

precision = [0.88, 0.59, 0.66, 0.97, 0.83, 0.67, 0.00, 0.97, 0.97, 0.85, 0.91, 0.91, 0.85,
             0.93, 0.49, 0.78, 0.52, 0.84, 0.50, 0.97, 0.91, 1.00, 0.58, 0.93, 0.95, 0.90,
             0.83, 0.78, 0.72, 0.59, 0.60, 0.64, 0.89, 0.96, 0.00, 0.84, 0.84, 0.87, 0.97,
             0.86, 0.96, 0.57, 0.86, 0.90, 0.81, 0.52, 0.71, 0.97, 0.87, 0.93, 0.86, 0.60,
             0.33, 0.89, 0.94, 0.53, 0.89, 0.70, 0.90, 0.87, 0.66, 0.95, 0.96, 0.76, 0.90,
             0.73, 0.61, 0.88, 0.75, 0.90, 0.00, 0.75, 0.81, 0.80, 0.61, 0.87, 0.71, 0.96,
             0.00, 0.97, 0.76, 0.68, 0.92, 0.70, 0.89, 0.40, 0.90, 0.81, 0.82, 0.56, 0.89,
             0.98, 0.98, 0.96, 0.69, 0.80, 0.95, 0.79, 0.56, 0.54]

recall = [0.82, 0.38, 0.67, 1.00, 0.76, 0.67, 0.00, 0.99, 0.99, 0.91, 0.91, 0.94, 0.91,
          0.92, 0.74, 0.80, 0.48, 0.85, 0.32, 0.98, 0.86, 0.08, 0.90, 0.76, 0.91, 0.89,
          0.79, 0.74, 0.71, 0.79, 0.61, 0.19, 0.97, 0.98, 0.00, 0.74, 0.94, 0.86, 0.97,
          0.86, 0.94, 0.52, 0.98, 0.98, 0.91, 0.44, 0.23, 1.00, 0.48, 0.88, 0.90, 0.47,
          0.03, 0.89, 0.95, 0.52, 1.00, 0.46, 0.94, 0.79, 0.73, 0.98, 0.96, 0.93, 0.89,
          0.77, 0.68, 0.92, 0.29, 0.96, 0.00, 0.92, 0.82, 0.76, 0.80, 0.87, 0.89, 0.97,
          0.00, 0.97, 0.89, 0.70, 0.95, 0.62, 0.91, 0.23, 0.81, 0.85, 0.77, 0.51, 0.77,
          1.00, 0.94, 0.96, 0.48, 0.88, 0.98, 0.74, 0.40, 0.85]

f1_scores = [0.84, 0.46, 0.67, 0.99, 0.79, 0.67, 0.00, 0.98, 0.98, 0.88, 0.91, 0.93, 0.88,
             0.92, 0.59, 0.79, 0.50, 0.84, 0.39, 0.98, 0.88, 0.14, 0.70, 0.84, 0.93, 0.89,
             0.81, 0.76, 0.72, 0.68, 0.61, 0.30, 0.93, 0.97, 0.00, 0.79, 0.89, 0.86, 0.97,
             0.86, 0.95, 0.54, 0.92, 0.94, 0.86, 0.47, 0.34, 0.99, 0.62, 0.91, 0.88, 0.53,
             0.05, 0.89, 0.95, 0.53, 0.94, 0.55, 0.92, 0.83, 0.70, 0.97, 0.96, 0.84, 0.90,
             0.75, 0.64, 0.90, 0.41, 0.93, 0.00, 0.83, 0.81, 0.78, 0.69, 0.87, 0.79, 0.96,
             0.00, 0.97, 0.82, 0.69, 0.94, 0.66, 0.90, 0.29, 0.85, 0.83, 0.80, 0.54, 0.83,
             0.99, 0.96, 0.96, 0.56, 0.84, 0.97, 0.76, 0.47, 0.66]

support = [60, 42, 103, 107, 70, 102, 3, 67, 70, 58, 82, 165, 252, 168, 34, 75, 66, 78, 25,
           416, 49, 52, 97, 17, 44, 96, 61, 77, 41, 52, 44, 36, 394, 53, 57, 43, 205, 93,
           110, 51, 172, 165, 519, 115, 163, 32, 44, 186, 54, 76, 60, 66, 34, 55, 177, 79,
           63, 46, 118, 52, 75, 61, 409, 74, 74, 143, 37, 218, 21, 57, 8, 49, 96, 108, 95,
           52, 19, 310, 5, 67, 53, 37, 86, 74, 223, 62, 195, 161, 189, 86, 53, 61, 145, 56,
           23, 84, 115, 148, 87, 93]

# ============================================================================
# GRAPH 1: CLASS DISTRIBUTION WITH F1 SCORES
# ============================================================================
print("\n1. Creating Class Distribution & F1 Scores Graph...")

fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# Top subplot: Class distribution (support)
axes[0].bar(range(num_classes), support, color='steelblue', alpha=0.7)
axes[0].set_xlabel('Class Index', fontsize=12)
axes[0].set_ylabel('Number of Test Samples', fontsize=12)
axes[0].set_title('LEDGAR Dataset - Test Set Distribution (100 Provision Types)',
                  fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Bottom subplot: F1 scores per class
colors = ['green' if score > 0.8 else 'orange' if score > 0.6 else 'red'
          for score in f1_scores]
axes[1].bar(range(num_classes), f1_scores, color=colors, alpha=0.7)
axes[1].axhline(y=0.8, color='green', linestyle='--', label='Good (>0.8)')
axes[1].axhline(y=0.6, color='orange', linestyle='--', label='Acceptable (>0.6)')
axes[1].set_xlabel('Class Index', fontsize=12)
axes[1].set_ylabel('F1 Score', fontsize=12)
axes[1].set_title('Per-Class F1 Scores on Test Set', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/1_class_distribution_scores.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 1_class_distribution_scores.png")

# ============================================================================
# GRAPH 4: TRAINING & VALIDATION LOSS
# ============================================================================
print("\n4. Creating Training & Validation Loss Graph...")

epoch_nums = np.arange(1, epochs + 1)

plt.figure(figsize=(10, 6))
plt.plot(epoch_nums, train_loss, marker='o', linewidth=2,
         label='Training Loss', color='blue', markersize=8)
plt.plot(epoch_nums, val_loss, marker='s', linewidth=2,
         label='Validation Loss', color='red', markersize=8)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training & Validation Loss Over Epochs',
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.xticks(epoch_nums)
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/4_training_validation_loss.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 4_training_validation_loss.png")

# ============================================================================
# GRAPH 5: TRAINING & VALIDATION ACCURACY
# ============================================================================
print("\n5. Creating Training & Validation Accuracy Graph...")

plt.figure(figsize=(10, 6))
plt.plot(epoch_nums, train_acc, marker='o', linewidth=2,
         label='Training Accuracy', color='green', markersize=8)
plt.plot(epoch_nums, val_acc, marker='s', linewidth=2,
         label='Validation Accuracy', color='orange', markersize=8)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Training & Validation Accuracy Over Epochs',
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.xticks(epoch_nums)
plt.ylim([0.6, 1.0])
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/5_training_validation_accuracy.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 5_training_validation_accuracy.png")

# ============================================================================
# GRAPH 7: PRECISION, RECALL, F1 COMPARISON (TOP 20 CLASSES BY SUPPORT)
# ============================================================================
print("\n7. Creating Precision, Recall, F1 Comparison...")

# Get top 20 classes by support
top_n = 20
top_indices = np.argsort(support)[-top_n:][::-1]
top_class_ids = [class_ids[i] for i in top_indices]
top_precision = [precision[i] for i in top_indices]
top_recall = [recall[i] for i in top_indices]
top_f1 = [f1_scores[i] for i in top_indices]

x = np.arange(top_n)
width = 0.25

fig, ax = plt.subplots(figsize=(14, 6))
bars1 = ax.bar(x - width, top_precision, width, label='Precision', color='skyblue')
bars2 = ax.bar(x, top_recall, width, label='Recall', color='lightgreen')
bars3 = ax.bar(x + width, top_f1, width, label='F1-Score', color='salmon')

ax.set_xlabel('Class', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Precision, Recall, and F1-Score (Top 20 Classes by Support)',
             fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels([f"C{cid}" for cid in top_class_ids], rotation=45, ha='right')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 1.0])

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/7_precision_recall_f1.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 7_precision_recall_f1.png")

# ============================================================================
# GRAPH 8: LEARNING RATE SCHEDULE
# ============================================================================
print("\n8. Creating Learning Rate Schedule Graph...")

# Based on your training setup
total_steps = 6750
warmup_steps = 405
max_lr = 2e-5

# Create learning rate schedule
steps = np.arange(total_steps)
lr_schedule = np.zeros(total_steps)

# Warmup phase
lr_schedule[:warmup_steps] = np.linspace(0, max_lr, warmup_steps)

# Linear decay
lr_schedule[warmup_steps:] = np.linspace(max_lr, 0, total_steps - warmup_steps)

plt.figure(figsize=(12, 6))
plt.plot(steps, lr_schedule, linewidth=2, color='purple')
plt.axvline(x=warmup_steps, color='red', linestyle='--',
            label=f'Warmup Steps ({warmup_steps})')
plt.xlabel('Training Steps', fontsize=12)
plt.ylabel('Learning Rate', fontsize=12)
plt.title('Learning Rate Schedule (Warmup + Linear Decay)',
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/8_learning_rate_schedule.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 8_learning_rate_schedule.png")

# ============================================================================
# GRAPH 9: MACRO-F1 vs ACCURACY OVER EPOCHS
# ============================================================================
print("\n9. Creating Accuracy vs Macro-F1 Comparison...")

# Calculate macro-F1 per epoch (approximation based on final metrics)
# Final macro-F1 is 0.7415, we'll estimate progression
macro_f1_epochs = [0.65, 0.71, 0.74]  # Estimated based on trend

fig, ax1 = plt.subplots(figsize=(10, 6))

color = 'tab:blue'
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Validation Accuracy', color=color, fontsize=12)
ax1.plot(epoch_nums, val_acc, marker='o', linewidth=2,
         color=color, markersize=8, label='Val Accuracy')
ax1.tick_params(axis='y', labelcolor=color)
ax1.grid(True, alpha=0.3)

ax2 = ax1.twinx()
color = 'tab:red'
ax2.set_ylabel('Estimated Macro-F1', color=color, fontsize=12)
ax2.plot(epoch_nums, macro_f1_epochs, marker='s', linewidth=2,
         color=color, markersize=8, label='Macro-F1 (est.)')
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Validation Accuracy vs Macro-F1 Score Over Epochs',
          fontsize=14, fontweight='bold')
fig.tight_layout()
plt.savefig('/content/drive/MyDrive/9_accuracy_vs_macrof1.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Saved: 9_accuracy_vs_macrof1.png")

# ============================================================================
# SUMMARY STATISTICS TABLE
# ============================================================================
print("\n10. Creating Summary Statistics Table...")

summary_data = {
    'Metric': ['Test Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1-Score',
               'Weighted F1-Score', 'Best Val Loss', 'Training Samples', 'Test Samples',
               'Number of Classes', 'Epochs Trained'],
    'Value': [f'{test_accuracy:.4f}', f'{test_precision:.4f}', f'{test_recall:.4f}',
              f'{test_f1:.4f}', '0.83', f'{val_loss[-1]:.4f}',
              f'{train_samples:,}', f'{test_samples:,}', str(num_classes), str(epochs)]
}

summary_df = pd.DataFrame(summary_data)
print("\nModel Performance Summary:")
print(summary_df.to_string(index=False))

# Save as CSV
summary_df.to_csv('/content/drive/MyDrive/model_summary_statistics.csv', index=False)
print("\n✓ Saved: model_summary_statistics.csv")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "="*70)
print("ALL VISUALIZATIONS COMPLETED WITH REAL DATA!")
print("="*70)
print("\nGenerated Graphs:")
print("1. ✓ Class Distribution & F1 Scores (Real Test Data)")
print("4. ✓ Training & Validation Loss (Real Training Data)")
print("5. ✓ Training & Validation Accuracy (Real Training Data)")
print("7. ✓ Precision, Recall, F1 Comparison (Top 20 Classes)")
print("8. ✓ Learning Rate Schedule (Your Training Config)")
print("9. ✓ Accuracy vs Macro-F1")
print("10. ✓ Summary Statistics Table")
print("\nAll graphs saved to Google Drive!")
print("\n📊 Final Test Results:")
print(f"   Accuracy: {test_accuracy:.2%}")
print(f"   Macro F1: {test_f1:.4f}")
print(f"   Best Val Loss: {val_loss[-1]:.4f}")
print("="*70)

"""# ✅ Task 7: Legal-BERT Training Pipeline Description

This notebook executes a **memory-optimized and ultra-fast legal clause classification pipeline** using the LEDGAR dataset (100 provision types). The system trains a transformer-based classifier **Legal-BERT** along with embedding-powered **Bi-LSTM models** (Word2Vec, GloVe, FastText) while applying aggressive performance optimizations to prevent RAM/GPU crashes.

---

## 🧠 **Pipeline Overview**

The workflow processes text as follows:

1. **Load dataset efficiently**
   - Only required columns (`clean_text`, `label`) are loaded.
   - Null rows are dropped immediately.
   - A 50K sample subset is selected for faster training.

2. **Encode labels**
   - `label` strings → `label_encoded` numeric IDs via `LabelEncoder`.
   - 100 unique provision types form the final class space.

3. **Stratified split**
   - Dataset divided into:
     - **Train (80%)**
     - **Validation (10%)**
     - **Test (20%)**

4. **Tokenization caching (for BERT)**
   - All splits are **pre-tokenized and stored in memory** before training.
   - Fixed max length of **128 tokens** for extreme speed.

5. **Create optimized DataLoaders**
   - Small batch size (8) + gradient accumulation (2) → effective batch 16.
   - `pin_memory`, parallel workers, and `non_blocking` transfers used.

6. **Load model**
   - Base model: `nlpaueb/legal-bert-base-uncased`
   - Modified for 100 labels and moved to available device (CPU/CUDA).
   - Gradient checkpointing enabled to reduce memory footprint.

7. **Train model in FP16 (if GPU available)**
   - Runs for **3 epochs** to maximize speed.
   - Gradient clipping prevents instability.
   - Scheduler warms up 6% of total steps then decays linearly.

8. **Evaluate on validation periodically**
   - Loss and accuracy tracked at every epoch.
   - Best model stored using lowest validation loss.

9. **Test set evaluation**
   - Generates:
     - **Accuracy score**
     - **Precision, Recall, F1 (macro)**
     - **Full classification report**

10. **Save outputs**
   - Final torch checkpoint: `legal_bert_final.pt`
   - Training history CSV
   - Loss/accuracy comparison plot image

---

## 🧩 **Model Behavior**

- Input text is tokenized into numerical sequences.
- The model embeds each word using:
  - **Contextual BERT vectors** (for transformer)
  - OR static embeddings (for LSTM variants)
- LSTM captures sequential dependencies.
- BERT captures semantic meaning bidirectionally via attention.
- Output is a probability distribution over **100 provisions**, selecting the highest as final prediction.

---

## ⚡ Optimizations Used

| Technique | Purpose |
|--------|--------|
| Sampling 50K data | Faster model convergence |
| Max token length 128 | Massive speed/memory gain |
| Batch 8 + Grad accum 2 | Stable training without OOM |
| FP16 mixed precision | Lower GPU memory demand |
| Gradient checkpointing | Freezes intermediate states |
| `pin_memory`, parallel workers, non-blocking GPU transfers | Faster data feeding |
| Explicit empty `color` usage, clean callbacks, GC & cache clearing | Prevents Colab RAM crash |

---

## 🎯 Expected Result

The trained classifier learns to map **LEDGAR legal text → correct provision type (0–99 IDs)** and return high confidence predictions for clear clauses such as:

- Governing Law
- Arbitration
- Confidentiality
- Termination
etc.

---

## ✅ Final Output

A **trained legal clause classification model** ready for inference, interactive testing, and batch predictions with real clause names via label mapping.

---

📌 You can now use `predict_legal_text(text, model_type='bert')` to run predictions.


"""

"""
LEDGAR Model Training Pipeline - FAST & MEMORY OPTIMIZED
Ultra-fast training with aggressive optimizations
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from tqdm import tqdm
from torch.optim import AdamW
import warnings
import gc
import os
warnings.filterwarnings('ignore')

print("="*70)
print("LEDGAR TRAINING PIPELINE - ULTRA FAST MODE")
print("="*70)

# ============================================================================
# AGGRESSIVE SPEED OPTIMIZATION CONFIG
# ============================================================================
CONFIG = {
    'data_path': '/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv',
    'text_column': 'clean_text',
    'label_column': 'label',
    'max_length': 128,  # Drastically reduced for speed
    'batch_size': 8,  # Optimized balance
    'gradient_accumulation_steps': 2,  # Effective batch: 16
    'learning_rate': 3e-5,  # Slightly higher for faster convergence
    'epochs': 3,  # Reduced from 5
    'test_size': 0.2,
    'val_size': 0.1,
    'random_state': 42,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'fp16': True,
    'sample_data': 50000,  # Use subset for faster training (set to None for full data)
    'save_dir': '/content/drive/MyDrive/legal_bert_models/',
    'num_workers': 2,  # Parallel data loading
    'eval_every_n_batches': 200,  # Less frequent validation
}

print(f"\n✓ Device: {CONFIG['device']}")
print(f"✓ Effective Batch Size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}")
print(f"✓ Max Length: {CONFIG['max_length']} tokens")
print(f"✓ FP16: {CONFIG['fp16']}")
print(f"✓ Sample Size: {CONFIG['sample_data'] if CONFIG['sample_data'] else 'Full Dataset'}")

# Clear GPU
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    gc.collect()
    print(f"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# ============================================================================
# FAST DATA LOADING
# ============================================================================
print("\n" + "="*70)
print("STEP 1: LOADING DATA")
print("="*70)

# Load only needed columns
print("Loading CSV...")
df = pd.read_csv(
    CONFIG['data_path'],
    usecols=[CONFIG['text_column'], CONFIG['label_column']],
    dtype={CONFIG['text_column']: str, CONFIG['label_column']: str}
)

# Drop nulls immediately
df = df.dropna(subset=[CONFIG['text_column'], CONFIG['label_column']])

# Sample for speed if configured
if CONFIG['sample_data'] and len(df) > CONFIG['sample_data']:
    df = df.sample(n=CONFIG['sample_data'], random_state=CONFIG['random_state']).reset_index(drop=True)
    print(f"✓ Sampled {len(df)} samples for faster training")
else:
    print(f"✓ Using all {len(df)} samples")

# Encode labels
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df[CONFIG['label_column']])
num_classes = len(label_encoder.classes_)
print(f"✓ Classes: {num_classes}")

# Fast stratified split
train_df, test_df = train_test_split(
    df, test_size=CONFIG['test_size'],
    random_state=CONFIG['random_state'],
    stratify=df['label_encoded']
)

train_df, val_df = train_test_split(
    train_df, test_size=CONFIG['val_size'],
    random_state=CONFIG['random_state'],
    stratify=train_df['label_encoded']
)

print(f"✓ Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

# ============================================================================
# FAST TOKENIZATION & CACHING
# ============================================================================
print("\n" + "="*70)
print("STEP 2: PRE-TOKENIZING (CACHE FOR SPEED)")
print("="*70)

tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')

def fast_tokenize_dataset(df, desc="Tokenizing"):
    """Pre-tokenize entire dataset for speed"""
    tokenized = []
    for text in tqdm(df[CONFIG['text_column']].values, desc=desc):
        tokens = tokenizer(
            str(text),
            max_length=CONFIG['max_length'],
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        tokenized.append({
            'input_ids': tokens['input_ids'].squeeze(),
            'attention_mask': tokens['attention_mask'].squeeze()
        })
    return tokenized

# Pre-tokenize all splits (faster during training)
print("Pre-tokenizing training data...")
train_tokens = fast_tokenize_dataset(train_df, "Train")
print("Pre-tokenizing validation data...")
val_tokens = fast_tokenize_dataset(val_df, "Val")
print("Pre-tokenizing test data...")
test_tokens = fast_tokenize_dataset(test_df, "Test")

class FastDataset(Dataset):
    """Dataset with pre-tokenized data"""
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokens[idx]['input_ids'],
            'attention_mask': self.tokens[idx]['attention_mask'],
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

train_dataset = FastDataset(train_tokens, train_df['label_encoded'].values)
val_dataset = FastDataset(val_tokens, val_df['label_encoded'].values)
test_dataset = FastDataset(test_tokens, test_df['label_encoded'].values)

train_loader = DataLoader(
    train_dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=True,
    num_workers=CONFIG['num_workers'],
    pin_memory=True,
    persistent_workers=True if CONFIG['num_workers'] > 0 else False
)
val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'] * 2, num_workers=CONFIG['num_workers'])
test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'] * 2, num_workers=CONFIG['num_workers'])

print(f"✓ Dataloaders ready!")

# Clear memory
del train_df, val_df, test_df
gc.collect()

# ============================================================================
# LOAD MODEL
# ============================================================================
print("\n" + "="*70)
print("STEP 3: LOADING MODEL")
print("="*70)

model = AutoModelForSequenceClassification.from_pretrained(
    'nlpaueb/legal-bert-base-uncased',
    num_labels=num_classes
)
model = model.to(CONFIG['device'])

# Gradient checkpointing
if hasattr(model, 'gradient_checkpointing_enable'):
    model.gradient_checkpointing_enable()

print("✓ Model loaded with gradient checkpointing!")

# ============================================================================
# TRAINING SETUP
# ============================================================================
print("\n" + "="*70)
print("STEP 4: TRAINING SETUP")
print("="*70)

optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)

total_steps = (len(train_loader) // CONFIG['gradient_accumulation_steps']) * CONFIG['epochs']
warmup_steps = int(0.06 * total_steps)  # Reduced warmup

scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)
criterion = nn.CrossEntropyLoss()
scaler = torch.cuda.amp.GradScaler() if CONFIG['fp16'] and CONFIG['device'] == 'cuda' else None

print(f"✓ Training steps: {total_steps} | Warmup: {warmup_steps}")

# ============================================================================
# ULTRA-FAST TRAINING LOOP
# ============================================================================
def train_epoch_fast(model, loader, optimizer, scheduler, criterion, device, scaler, grad_accum):
    model.train()
    total_loss, correct, total = 0, 0, 0
    optimizer.zero_grad()

    for step, batch in enumerate(tqdm(loader, desc="Training")):
        input_ids = batch['input_ids'].to(device, non_blocking=True)
        attention_mask = batch['attention_mask'].to(device, non_blocking=True)
        labels = batch['labels'].to(device, non_blocking=True)

        if scaler:
            with torch.cuda.amp.autocast():
                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs.logits, labels) / grad_accum
            scaler.scale(loss).backward()
        else:
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs.logits, labels) / grad_accum
            loss.backward()

        if (step + 1) % grad_accum == 0:
            if scaler:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        total_loss += loss.item() * grad_accum
        preds = torch.argmax(outputs.logits, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return total_loss / len(loader), correct / total

def evaluate_fast(model, loader, criterion, device):
    model.eval()
    total_loss, correct, total = 0, 0, 0

    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = batch['labels'].to(device, non_blocking=True)

            outputs = model(input_ids, mask)
            loss = criterion(outputs.logits, labels)

            total_loss += loss.item()
            preds = torch.argmax(outputs.logits, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return total_loss / len(loader), correct / total

# ============================================================================
# TRAINING
# ============================================================================
print("\n" + "="*70)
print("STEP 5: TRAINING (FAST MODE)")
print("="*70)

history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
best_val_loss = float('inf')
best_model_path = f"{CONFIG['save_dir']}best_model.pt"
os.makedirs(CONFIG['save_dir'], exist_ok=True)

for epoch in range(CONFIG['epochs']):
    print(f"\n{'='*50}")
    print(f"Epoch {epoch+1}/{CONFIG['epochs']}")
    print(f"{'='*50}")

    train_loss, train_acc = train_epoch_fast(
        model, train_loader, optimizer, scheduler, criterion,
        CONFIG['device'], scaler, CONFIG['gradient_accumulation_steps']
    )

    val_loss, val_acc = evaluate_fast(model, val_loader, criterion, CONFIG['device'])

    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_acc'].append(train_acc)
    history['val_acc'].append(val_acc)

    print(f"\n📊 Results:")
    print(f"   Train → Loss: {train_loss:.4f} | Acc: {train_acc:.4f}")
    print(f"   Val   → Loss: {val_loss:.4f} | Acc: {val_acc:.4f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), best_model_path)
        print("   ✓ Best model saved!")

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\n✓ Training complete!")

# Load best model
model.load_state_dict(torch.load(best_model_path))

# ============================================================================
# TEST EVALUATION
# ============================================================================
print("\n" + "="*70)
print("STEP 6: TEST EVALUATION")
print("="*70)

test_loss, test_acc = evaluate_fast(model, test_loader, criterion, CONFIG['device'])
print(f"\n🎯 Test Accuracy: {test_acc:.4f}")

# Detailed metrics
all_preds, all_true = [], []
model.eval()
with torch.no_grad():
    for batch in tqdm(test_loader, desc="Predictions"):
        outputs = model(
            batch['input_ids'].to(CONFIG['device']),
            batch['attention_mask'].to(CONFIG['device'])
        )
        preds = torch.argmax(outputs.logits, 1).cpu().numpy()
        all_preds.extend(preds)
        all_true.extend(batch['labels'].numpy())

precision, recall, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='macro')
print(f"\n📈 Metrics:")
print(f"   Precision: {precision:.4f}")
print(f"   Recall: {recall:.4f}")
print(f"   F1-Score: {f1:.4f}")

print(f"\n{classification_report(all_true, all_preds, target_names=label_encoder.classes_)}")

# ============================================================================
# SAVE EVERYTHING
# ============================================================================
print("\n" + "="*70)
print("STEP 7: SAVING")
print("="*70)

# Save final model with metadata
torch.save({
    'model': model.state_dict(),
    'config': CONFIG,
    'labels': label_encoder.classes_.tolist(),
    'history': history,
    'test_acc': test_acc,
    'test_f1': f1
}, f"{CONFIG['save_dir']}legal_bert_final.pt")

pd.DataFrame(history).to_csv(f"{CONFIG['save_dir']}history.csv", index=False)

print("✓ Model saved!")

# ============================================================================
# VISUALIZATIONS
# ============================================================================
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
epochs = range(1, CONFIG['epochs'] + 1)

axes[0].plot(epochs, history['train_loss'], 'o-', label="Train", linewidth=2)
axes[0].plot(epochs, history['val_loss'], 's-', label="Val", linewidth=2)
axes[0].set_title("Loss", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Epoch")
axes[0].set_ylabel("Loss")
axes[0].legend()
axes[0].grid(alpha=0.3)

axes[1].plot(epochs, history['train_acc'], 'o-', label="Train", linewidth=2)
axes[1].plot(epochs, history['val_acc'], 's-', label="Val", linewidth=2)
axes[1].set_title("Accuracy", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Epoch")
axes[1].set_ylabel("Accuracy")
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(f"{CONFIG['save_dir']}training_plots.png", dpi=150)
plt.show()

print("\n" + "="*70)
print("✅ COMPLETE!")
print("="*70)
print(f"Final Test Accuracy: {test_acc:.4f}")
print(f"Best Val Loss: {best_val_loss:.4f}")
print(f"Models saved to: {CONFIG['save_dir']}")

"""# 🧪 Legal-BERT Inference Script — What This Script Does

This Python script provides a **user-driven inference interface** for your trained *Legal-BERT* text classifier.  
Its purpose is to **load the trained model and classify any legal clause entered by the user**, returning the most probable provision type along with confidence scores.

---

## ⚙️ Script Functionality

1. **Loads the original LEDGAR dataset**  
   - Reads the CSV file containing `clean_text`, `label`, and `label_name`  
   - Extracts the mapping between **class IDs → real legal provision names (100 types)**

2. **Loads your trained Legal-BERT model checkpoint**  
   - File used: `legal_bert_final.pt` (saved after training)  
   - Restores:
     - The model weights
     - The list of labels (class IDs)
     - Evaluation stats such as **test accuracy and F1-score**
   - Sets device to CPU or GPU automatically

3. **Rebuilds the Label Encoder**  
   - Converts predicted numeric class index back into the original **class ID string**
   - Matches that class ID to the real provision name using dataset mapping

4. **Defines prediction functions**  
   - `predict_legal_provision(text)`
     → Tokenizes input and outputs top-K provision predictions
   - `batch_predict(texts)`
     → Runs inference on multiple clauses at once

5. **Runs in Interactive Mode (terminal-like input)**  
   Allows the user to:
   - Enter any **legal clause** for classification
   - Use commands:
     - `stats` → view model performance
     - `classes` → list provision types
     - `batch` → classify multiple clauses
     - `quit` → exit

---

## 🎯 Example Test Result (from the example)
- **Predicted Provision**: *Confidentiality*
- **Class ID**: `20`
- **Confidence**: `96.35%`
- **Model Performance**:
  - Accuracy: `84.00%`
  - Macro F1-Score: `74.15%`

---

✅ **Conclusion**:  
This model successfully classifies legal text using real provision names and confidence-based ranking, and the script is now ready to evaluate any user-provided clause without crashing Colab.

"""

"""
Enhanced Inference Script for Trained Legal-BERT
Loads actual provision names from the original dataset!
"""

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("LEGAL-BERT INFERENCE - WITH REAL PROVISION NAMES")
print("="*70)

# ============================================================================
# LOAD CLASS NAMES FROM ORIGINAL DATASET
# ============================================================================
print("\nStep 1: Loading class name mapping from dataset...")

# Path to your original dataset with provision names
DATASET_PATH = '/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv'

try:
    # Load the dataset to get the mapping between class IDs and provision names
    df = pd.read_csv(DATASET_PATH)

    print(f"✓ Dataset loaded: {len(df)} rows")
    print(f"✓ Columns found: {', '.join(df.columns.tolist())}")

    label_col = 'label'
    name_col = 'label_name'

    if label_col in df.columns and name_col in df.columns:
        class_name_mapping = df.groupby(label_col)[name_col].first().to_dict()
        print(f"✓ Using columns: '{label_col}' (IDs) → '{name_col}' (Names)")
        print(f"✓ Loaded {len(class_name_mapping)} class names")
    else:
        print("⚠️ Required columns not found, continuing with IDs only...")
        class_name_mapping = None

except FileNotFoundError:
    print("⚠️ Dataset not found, using class IDs only...")
    class_name_mapping = None
except Exception as e:
    print(f"⚠️ Error loading class names: {e}")
    class_name_mapping = None

# ============================================================================
# LOAD TRAINED MODEL
# ============================================================================
print("\nStep 2: Loading your trained model...")

MODEL_PATH = '/content/drive/MyDrive/legal_bert_models/legal_bert_final.pt'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)

    labels_list = checkpoint['labels']
    num_classes = len(labels_list)

    label_encoder = LabelEncoder()
    label_encoder.classes_ = np.array(labels_list)

    tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')

    model = AutoModelForSequenceClassification.from_pretrained(
        'nlpaueb/legal-bert-base-uncased',
        num_labels=num_classes
    )
    model.load_state_dict(checkpoint['model'])
    model = model.to(device)
    model.eval()

    print("✓ Model loaded successfully!")
    print(f"✓ Device: {device}")
    print(f"✓ Classes: {num_classes}")
    print(f"✓ Test Accuracy: {checkpoint['test_acc']:.2%}")
    print(f"✓ Test F1-Score: {checkpoint['test_f1']:.2%}")

except Exception as e:
    print(f"❌ Error loading model: {e}")
    exit()

# ============================================================================
# HELPER FUNCTION TO GET PROVISION NAME
# ============================================================================
def get_provision_name(class_id_str):
    if class_name_mapping is None:
        return f"Class {class_id_str}"

    try:
        class_id_int = int(class_id_str)
        return class_name_mapping.get(class_id_int, f"Class {class_id_str}")
    except:
        return f"Class {class_id_str}"

# ============================================================================
# PREDICTION FUNCTION
# ============================================================================
def predict_legal_provision(text, top_k=5):
    inputs = tokenizer(
        text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=1)[0]

    top_probs, top_indices = torch.topk(probabilities, top_k)

    predictions = []
    for prob, idx in zip(top_probs, top_indices):
        class_id_str = label_encoder.inverse_transform([idx.item()])[0]
        class_name = get_provision_name(class_id_str)

        predictions.append({
            'class_id': class_id_str,
            'class_name': class_name,
            'confidence': prob.item()
        })

    return {
        'top_prediction': {
            'id': predictions[0]['class_id'],
            'name': predictions[0]['class_name'],
            'confidence': predictions[0]['confidence']
        },
        'all_predictions': predictions
    }

# ============================================================================
# BATCH PREDICTION FUNCTION
# ============================================================================
def batch_predict(texts, show_details=False):
    results = []

    print(f"\n{'='*70}")
    print(f"BATCH PREDICTION - {len(texts)} texts")
    print(f"{'='*70}\n")

    for i, text in enumerate(texts, 1):
        result = predict_legal_provision(text, top_k=3)
        top = result['top_prediction']
        results.append(result)

        print(f"{i}. {text[:60]}...")
        print(f"   → {top['name']} (ID: {top['id']}, {top['confidence']:.1%})\n")

        if show_details:
            for j, pred in enumerate(result['all_predictions'][1:], 2):
                print(f"      {j}. {pred['class_name']} ({pred['confidence']:.1%})")
        print()

    return results

# ============================================================================
# INTERACTIVE MODE (only asks user input now - no examples!)
# ============================================================================
print("\n" + "="*70)
print("INTERACTIVE MODE")
print("="*70)
print("Enter legal text to classify (No examples are hard-coded)")
print("\nCommands:")
print("  - Type your legal clause to classify")
print("  - 'batch' - Enter multiple texts")
print("  - 'stats' - Show model statistics")
print("  - 'classes' - Show all provision types")
print("  - 'quit' - Exit")
print("="*70)

while True:
    print("\n" + "-"*70)
    user_input = input("\nEnter legal text (or command): ").strip()

    if user_input.lower() in ['quit', 'exit', 'q']:
        print("\n👋 Exiting...")
        break

    if user_input.lower() == 'stats':
        print("\n📊 MODEL STATISTICS:")
        print(f"   Test Accuracy: {checkpoint['test_acc']:.2%}")
        print(f"   Test F1-Score: {checkpoint['test_f1']:.2%}")
        print(f"   Number of Classes: {num_classes}")
        print(f"   Model: Legal-BERT")
        print(f"   Device: {device}")
        continue

    if user_input.lower() == 'classes':
        if class_name_mapping:
            print("\n📋 ALL PROVISION TYPES:")
            for class_id, name in sorted(class_name_mapping.items())[:20]:
                print(f"   {class_id:3d}: {name}")
            if len(class_name_mapping) > 20:
                print(f"   ... and {len(class_name_mapping) - 20} more")
        else:
            print("\n⚠️ Class names not available")
        continue

    if user_input.lower() == 'batch':
        print("\nEnter multiple texts (one per line).")
        print("Type 'done' when finished:")
        batch_texts = []
        while True:
            text = input(f"  Text {len(batch_texts) + 1}: ").strip()
            if text.lower() == 'done':
                break
            if text:
                batch_texts.append(text)

        if batch_texts:
            show_details = input("\nShow all predictions? (y/n): ").lower() == 'y'
            batch_predict(batch_texts, show_details=show_details)
        continue

    if not user_input:
        print("⚠️ Please enter some text")
        continue

    if len(user_input) < 20:
        print("⚠️ Text too short. Please enter at least 20 characters.")
        continue

    # Predict
    result = predict_legal_provision(user_input, top_k=5)
    top = result['top_prediction']

    print("\n🎯 PREDICTION:")
    print(f"   Provision Type: {top['name']}")
    print(f"   Class ID: {top['id']}")
    print(f"   Confidence: {top['confidence']:.2%}")

    show_all = input("\nShow top 5 predictions? (y/n): ").lower()
    if show_all == 'y':
        print("\n📊 Top 5 Predictions:")
        print(f"   {'Rank':<6} {'ID':<8} {'Confidence':<12} {'Provision Type'}")
        print(f"   {'-'*6} {'-'*8} {'-'*12} {'-'*60}")
        for j, pred in enumerate(result['all_predictions'], 1):
            bar = '█' * int(pred['confidence'] * 30)
            print(f"   {j:<6} {pred['class_id']:<8} {pred['confidence']:>6.2%}  {bar:<30} {pred['class_name'][:60]}")

print("\n" + "="*70)
print("✅ INFERENCE TESTING COMPLETE!")
print("="*70)
print("\nModel is ready to classify legal text!")
print(f"Recognizes {num_classes} provision types.")
print("="*70)

"""
LEDGAR Legal Document Classifier - Clean Colab Version
Now with Class Names from Dataset
"""

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import gradio as gr
import warnings
warnings.filterwarnings('ignore')

# ======================================================================
# LOAD CLASS NAMES FROM ORIGINAL DATASET
# ======================================================================

DATASET_PATH = "/content/drive/MyDrive/LEDGAR_final_clean_with_names.csv"
class_name_mapping = None

try:
    df = pd.read_csv(DATASET_PATH)

    # Must have columns: label, label_name
    if "label" in df.columns and "label_name" in df.columns:
        class_name_mapping = df.groupby("label")["label_name"].first().to_dict()
        print(f"Loaded {len(class_name_mapping)} class names.")
    else:
        print("⚠️ Dataset missing required columns. Only IDs will be shown.")
except:
    print("⚠️ Could not load class names. Only IDs will be shown.")

# Helper function
def get_provision_name(class_id):
    try:
        class_id = int(class_id)
        return class_name_mapping.get(class_id, f"Class {class_id}")
    except:
        return f"Class {class_id}"


# ======================================================================
# LOAD MODEL
# ======================================================================

MODEL_PATH = "/content/drive/MyDrive/legal_bert_models/legal_bert_final.pt"
device = "cuda" if torch.cuda.is_available() else "cpu"

print("Loading model...")

try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)

    labels_list = checkpoint["labels"]
    num_classes = len(labels_list)

    label_encoder = LabelEncoder()
    label_encoder.classes_ = np.array(labels_list)

    tokenizer = AutoTokenizer.from_pretrained("nlpaueb/legal-bert-base-uncased")

    model = AutoModelForSequenceClassification.from_pretrained(
        "nlpaueb/legal-bert-base-uncased",
        num_labels=num_classes
    )
    model.load_state_dict(checkpoint["model"])
    model.to(device)
    model.eval()

    print("Model loaded successfully!")

except Exception as e:
    print("ERROR loading model:", e)
    model = None

# ======================================================================
# PREDICTION FUNCTION (WITH LABEL NAMES)
# ======================================================================

def predict(text, top_n=5):
    if model is None:
        return "❌ Model not loaded."

    if not text or len(text.strip()) < 20:
        return "⚠️ Enter at least 20 characters."

    inputs = tokenizer(
        text,
        max_length=256,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.softmax(outputs.logits, dim=1)[0]

    top_probs, top_idx = torch.topk(probs, min(top_n, len(probs)))

    results = []
    for i in range(len(top_probs)):
        label_id = label_encoder.inverse_transform([top_idx[i].item()])[0]
        conf = float(top_probs[i].item())

        # Get readable label name
        label_name = get_provision_name(label_id)

        results.append(
            f"{i+1}. ID: {label_id} — {label_name} — {conf:.2%}"
        )

    return "\n".join(results)

# ======================================================================
# GRADIO INTERFACE
# ======================================================================

demo = gr.Interface(
    fn=predict,
    inputs=[
        gr.Textbox(label="Legal Text", lines=8, placeholder="Enter contract text..."),
        gr.Slider(3, 10, value=5, step=1, label="Top N Predictions")
    ],
    outputs=gr.Textbox(label="Predicted Classes"),
    title="LEDGAR Legal-BERT Classifier (With Provision Names)"
)

# ======================================================================
# LAUNCH
# ======================================================================

demo.launch(
    share=True,    # REQUIRED FOR COLAB
    debug=False,
    show_error=True
)